{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neurons and Neural Networks\n",
    "This notebook includes implementations of artificial neurons (Perceptron, Adaline & Logistic Regression) and simple neural networks from scratch in Python.\n",
    "\n",
    "1) [The Perceptron](#artificial-neuron-1-the-perceptron)\n",
    "2) [Adaline](#artificial-neuron-2-adaline)\n",
    "3) [Logistic Regression](#artificial-neuron-3-logistic-regression)\n",
    "4) [Binary Classification with NLL Loss and ReLu activation](#binary-classification-with-nll-loss-and-relu-activation)\n",
    "5) [Binary Classification with NLL Loss and Sigmoid activation](#binary-classification-with-nll-loss-and-sigmoid-activation)\n",
    "6) [Multiclass Classification with Cross Entropy Loss](#multiclass-classification-with-cross-entropy-loss)\n",
    "7) [Multiclass Classification with Cross Entropy Loss and L2 Regularization](#multiclass-classification-with-cross-entropy-loss-and-l2-regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data=load_breast_cancer(as_frame=True)\n",
    "X,y=data.data,data.target\n",
    "# Since the default in the file is 0=malignant 1=benign we want to reverse these\n",
    "y=(y==0).astype(int)\n",
    "X,y= np.array(X),np.array(y)\n",
    "\n",
    "# Let's set aside a test set and use the remainder for training and cross-validation\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.2)\n",
    "\n",
    "data.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scale our data to help speed convergence\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neurons\n",
    "### Artificial Neuron 1: The Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self,eta=0.01,n_iter=100,random_state=0):\n",
    "        self.eta=eta\n",
    "        self.n_iter=n_iter\n",
    "        self.random_state=random_state\n",
    "        self.cost_path=[]\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        # Initialize the weights and bias (weights[0]) to small random numbers\n",
    "        rgen=np.random.RandomState(self.random_state)\n",
    "        self.weights = rgen.normal(loc=0.0,scale=0.01,size=(1+X.shape[1]))\n",
    "        \n",
    "        # Train the perceptron using stochastic gradient descent\n",
    "        for i in range(self.n_iter):\n",
    "            # Keep track of cost for each iteration\n",
    "            cost_total = 0\n",
    "            # Stochastic gradient descent\n",
    "            for xi,yi in zip(X,y):\n",
    "                # Get prediction\n",
    "                yhat = self.predict(xi)\n",
    "                # Calculate cost function - SSE\n",
    "                error = yi-yhat\n",
    "                cost = 0.5*(error)**2\n",
    "                gradient_weights = error*xi\n",
    "                gradient_bias = error*1\n",
    "                # Update rule\n",
    "                delta_weights = self.eta * gradient_weights\n",
    "                delta_bias = self.eta * gradient_bias\n",
    "                # Update the weights\n",
    "                self.weights[1:] += delta_weights\n",
    "                # Update bias\n",
    "                self.weights[0] += delta_bias\n",
    "                # Count errors\n",
    "                cost_total+= cost\n",
    "            #self.error_counts.append(error_count)\n",
    "            self.cost_path.append(cost_total)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        z = np.dot(X,self.weights[1:]) + self.weights[0]\n",
    "        if z >= 0.0:\n",
    "            yhat = 1\n",
    "        else:\n",
    "            yhat = 0\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_state = 123\n",
    "perceptron_model = Perceptron(eta=0.00001,n_iter=100, random_state=random_state)\n",
    "perceptron_model.fit(X_train_scaled,y_train)\n",
    "plt.plot(range(len(perceptron_model.cost_path)),perceptron_model.cost_path)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy is 0.930\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "for x in X_test_scaled:\n",
    "    pred = perceptron_model.predict(x)\n",
    "    test_preds.append(pred)\n",
    "test_acc = np.sum(test_preds==y_test)/len(y_test)\n",
    "print('Test set accuracy is {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neuron 2: Adaline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline:\n",
    "    \n",
    "    def __init__(self,eta=0.01,n_iter=100,random_state=0):\n",
    "        self.eta=eta\n",
    "        self.n_iter=n_iter\n",
    "        self.random_state=random_state\n",
    "        self.cost_path=[]\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        # Initialize the weights and bias (weights[0]) to small random numbers\n",
    "        rgen=np.random.RandomState(self.random_state)\n",
    "        self.weights = rgen.normal(loc=0.0,scale=0.01,size=(1+X.shape[1]))\n",
    "        \n",
    "        # Train adaline using batch gradient descent\n",
    "        for i in range(self.n_iter):\n",
    "            yhat = self.predict(X)\n",
    "            # Calculate the cost and gradient of cost with respect to weights and bias\n",
    "            cost = np.sum(0.5 * (y-yhat)**2)\n",
    "            gradient_weights = X.T.dot(y-yhat)\n",
    "            gradient_bias = np.sum(y-yhat)\n",
    "            # Update weights and bias\n",
    "            delta_weights = self.eta * gradient_weights\n",
    "            delta_bias = self.eta * gradient_bias\n",
    "            self.weights[1:] += delta_weights # Update weights\n",
    "            self.weights[0] += delta_bias # Update bias\n",
    "            # Add cost to total cost counter\n",
    "            self.cost_path.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        z = np.dot(X,self.weights[1:]) + self.weights[0]\n",
    "        yhat = 1*z\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "adaline_model = Adaline(eta=0.0001,n_iter=100, random_state=random_state)\n",
    "adaline_model.fit(X_train_scaled,y_train)\n",
    "plt.plot(range(len(adaline_model.cost_path)),adaline_model.cost_path)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy is 0.921\n"
     ]
    }
   ],
   "source": [
    "test_preds = adaline_model.predict(X_test_scaled)\n",
    "test_preds = np.round(test_preds)\n",
    "test_acc = np.sum(test_preds==y_test)/len(y_test)\n",
    "print('Test set accuracy is {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neuron 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "    \n",
    "    def __init__(self,eta=0.01,n_iter=100,random_state=0):\n",
    "        self.eta=eta\n",
    "        self.n_iter=n_iter\n",
    "        self.random_state=random_state\n",
    "        self.cost_path=[]\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        # Initialize the weights and bias (weights[0]) to small random numbers\n",
    "        rgen=np.random.RandomState(self.random_state)\n",
    "        self.weights = rgen.normal(loc=0.0,scale=0.01,size=(1+X.shape[1]))\n",
    "        \n",
    "        # Train adaline using batch gradient descent\n",
    "        for i in range(self.n_iter):\n",
    "            yhat = self.predict(X)\n",
    "            # Calculate the cost and gradient of cost with respect to weights and bias\n",
    "            cost = (-y.dot(np.log(yhat)) - ((1-y).dot(np.log(1-yhat))))\n",
    "            gradient_weights = -X.T.dot(y-yhat) # Gradient of cost wrt weights\n",
    "            gradient_bias = -np.sum(y-yhat) # Gradient of cost wrt bias\n",
    "            \n",
    "            # Update the weights and bias\n",
    "            delta_weights = self.eta * gradient_weights\n",
    "            delta_bias = self.eta * gradient_bias\n",
    "            self.weights[1:] -= delta_weights # Update weights\n",
    "            self.weights[0] -= delta_bias # Update bias\n",
    "            # Add cost to total cost counter\n",
    "            self.cost_path.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        z = np.dot(X,self.weights[1:]) + self.weights[0]\n",
    "        yhat = 1. / (1. + np.exp(-np.clip(z,-250,250)))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48221005 0.49506913 0.50378299 0.48395065 0.49639885 0.48945631\n",
      " 0.49724208 0.48265285 0.52720648 0.47742749 0.56068877 0.4882357\n",
      " 0.50107175 0.47454399 0.47176473 0.48472162 0.53235254 0.47206088\n",
      " 0.4847832  0.5049186  0.49092736 0.47919815 0.5138422  0.50849531\n",
      " 0.58497686 0.48037759 0.4858168  0.47565637 0.54847535 0.49241959\n",
      " 0.48251534 0.55644608 0.49656175 0.52646904 0.48813937 0.49286579\n",
      " 0.4956916  0.50294803 0.4817548  0.49674755 0.47576135 0.51157192\n",
      " 0.49643811 0.50965801 0.48906745 0.52005685 0.55351679 0.48833725\n",
      " 0.49181133 0.53349087 0.48521811 0.51942949 0.50827839 0.5280197\n",
      " 0.49848518 0.50136937 0.47437475 0.47916391 0.47169325 0.49371856\n",
      " 0.52598464 0.45217373 0.46924736 0.49092808 0.47491363 0.51138508\n",
      " 0.54661792 0.48790204 0.50769394 0.51082893 0.4788     0.47965782\n",
      " 0.49820959 0.48335245 0.54517268 0.48066148 0.47093849 0.53002896\n",
      " 0.51080306 0.47602809 0.4745856  0.53833777 0.56767813 0.49566797\n",
      " 0.49047272 0.52066223 0.49682846 0.47154772 0.52034854 0.56903706\n",
      " 0.54047601 0.4772957  0.49041187 0.45994737 0.54350464 0.49432934\n",
      " 0.49096649 0.4806578  0.45781347 0.48095833 0.57684283 0.48523678\n",
      " 0.55721221 0.48538056 0.53933814 0.49180063 0.50270372 0.48529783\n",
      " 0.50453695 0.49286223 0.50583306 0.4771087  0.49606677 0.52360982\n",
      " 0.5002038  0.55169317 0.49791131 0.48042358 0.49320461 0.56390921\n",
      " 0.49959406 0.48591887 0.49161018 0.52690331 0.46947497 0.50015078\n",
      " 0.525032   0.52662173 0.50254898 0.57021466 0.48241374 0.49836201\n",
      " 0.48139359 0.51616044 0.49273904 0.51827607 0.50282656 0.4945998\n",
      " 0.49622801 0.48702429 0.57617157 0.49226364 0.52487861 0.48255329\n",
      " 0.53135992 0.47448782 0.49765322 0.53890346 0.49420327 0.50135645\n",
      " 0.47573186 0.4894559  0.49278713 0.51558686 0.49596644 0.47809305\n",
      " 0.49482174 0.49233043 0.47487774 0.49878314 0.47748431 0.56932034\n",
      " 0.48650856 0.49204972 0.48574905 0.48450601 0.48536055 0.4899063\n",
      " 0.50139081 0.47706367 0.49459166 0.5010098  0.55763795 0.48998988\n",
      " 0.54495166 0.46980225 0.49704732 0.49978907 0.46636036 0.51567881\n",
      " 0.51774147 0.47790913 0.50690769 0.54650585 0.48930724 0.49231513\n",
      " 0.50843749 0.49074253 0.48470663 0.48025991 0.54370431 0.46330141\n",
      " 0.50133595 0.48877156 0.46626404 0.51197202 0.53871077 0.54308064\n",
      " 0.48048084 0.48575352 0.48381888 0.48312669 0.51929618 0.53826595\n",
      " 0.49298387 0.48752218 0.50085458 0.49824335 0.50038091 0.50357112\n",
      " 0.50437119 0.54654207 0.5000632  0.48461657 0.50502815 0.47302796\n",
      " 0.51818501 0.50400198 0.50164115 0.49329192 0.48651327 0.55263081\n",
      " 0.55566346 0.49209404 0.49510292 0.48619871 0.5303027  0.47930453\n",
      " 0.51798635 0.53421513 0.50024656 0.49493173 0.48434066 0.51043152\n",
      " 0.48782605 0.53201566 0.50949199 0.48516196 0.52957741 0.50210632\n",
      " 0.48766766 0.51943736 0.59932943 0.48398298 0.50264562 0.48753626\n",
      " 0.55109075 0.50646406 0.49083861 0.48860829 0.5207321  0.47427602\n",
      " 0.49393252 0.46799175 0.52930905 0.48233734 0.53776245 0.49713186\n",
      " 0.5449313  0.51708033 0.54726672 0.55391698 0.49527285 0.46200371\n",
      " 0.50611012 0.46498283 0.50772009 0.52403428 0.50845907 0.49129844\n",
      " 0.50333745 0.48325295 0.4849614  0.51808656 0.48370247 0.4842702\n",
      " 0.52642806 0.48838685 0.50230068 0.52801143 0.53621616 0.52687174\n",
      " 0.50876876 0.49564206 0.49811927 0.53840288 0.47191553 0.49692752\n",
      " 0.4492383  0.51256968 0.50300829 0.49366558 0.48436839 0.48681617\n",
      " 0.49437963 0.48035848 0.53994117 0.54531735 0.58846561 0.50496267\n",
      " 0.52341506 0.56045559 0.49614173 0.49094688 0.49106471 0.48460427\n",
      " 0.49256706 0.49193291 0.51583724 0.46799015 0.4943971  0.47962357\n",
      " 0.50802813 0.47167969 0.5030777  0.50691815 0.53585427 0.49265309\n",
      " 0.54040633 0.49311654 0.53860684 0.51000528 0.49573527 0.49614893\n",
      " 0.48101935 0.487047   0.48254569 0.55102877 0.52050952 0.5283547\n",
      " 0.48763276 0.47564144 0.5566842  0.52540957 0.48937182 0.51022158\n",
      " 0.51460175 0.49025593 0.50727871 0.50664498 0.49059205 0.48571902\n",
      " 0.49492195 0.48246183 0.48980694 0.49467122 0.49355247 0.48271793\n",
      " 0.48261889 0.57849415 0.53073681 0.48985979 0.4747769  0.48597868\n",
      " 0.52134846 0.48574103 0.50637773 0.48009456 0.48326965 0.55964677\n",
      " 0.48978343 0.51733145 0.50706935 0.53533819 0.49189812 0.54643542\n",
      " 0.48315975 0.52432254 0.48523164 0.56708829 0.64232922 0.4745497\n",
      " 0.47753274 0.49782362 0.48534122 0.48423284 0.47768819 0.48268594\n",
      " 0.49703733 0.56160638 0.5604395  0.51634091 0.53183252 0.55304551\n",
      " 0.50926302 0.48870084 0.50024631 0.47945502 0.50377393 0.47932078\n",
      " 0.48351301 0.53918853 0.49523477 0.52313566 0.49906718 0.49101518\n",
      " 0.50665306 0.49917142 0.49643337 0.59249012 0.50953527 0.50101887\n",
      " 0.48638612 0.51825432 0.48964308 0.4790359  0.51132089 0.58432244\n",
      " 0.52086348 0.53104257 0.5092892  0.51710467 0.54128698 0.50714762\n",
      " 0.48624415 0.56864714 0.47757084 0.52700431 0.47363117 0.48143399\n",
      " 0.52522485 0.50635251 0.50268048 0.52885019 0.47709433 0.49170457\n",
      " 0.48147525 0.4706521  0.5376582  0.50914113 0.49428532 0.58373786\n",
      " 0.5114364  0.50238975 0.50253994 0.52065248 0.50851019 0.5060532\n",
      " 0.48146164 0.48619805 0.53645269 0.49398924 0.48164949 0.54294413\n",
      " 0.49094888 0.50029415 0.46102939 0.49868379 0.50290955 0.54270196\n",
      " 0.51950529 0.51191251 0.47677179 0.48810504 0.49645924]\n",
      "[0.12392839 0.20022048 0.37550817 0.15196998 0.17578078 0.20462523\n",
      " 0.16288887 0.15145457 0.96375688 0.31848765 0.97895443 0.50466367\n",
      " 0.3818902  0.1185363  0.12844324 0.05496338 0.87196643 0.07405714\n",
      " 0.09893799 0.54408393 0.57626357 0.07715669 0.36658302 0.43606088\n",
      " 0.99616137 0.21083488 0.14516261 0.28816695 0.98195932 0.1610432\n",
      " 0.11282573 0.93471665 0.13246553 0.49407678 0.12322631 0.14281981\n",
      " 0.29075906 0.80861725 0.80003752 0.41471259 0.09871113 0.77039827\n",
      " 0.22159452 0.48820219 0.0919277  0.92379717 0.96179123 0.09828385\n",
      " 0.38905576 0.84543442 0.12586012 0.89764609 0.75986267 0.86599419\n",
      " 0.39761103 0.20338433 0.07766932 0.18298768 0.09135564 0.1944696\n",
      " 0.67008301 0.04064115 0.06924434 0.27552514 0.08515547 0.4830358\n",
      " 0.97871147 0.26734737 0.32467538 0.48897268 0.15750531 0.09626292\n",
      " 0.34265065 0.26788959 0.95714512 0.06483407 0.1341306  0.84919705\n",
      " 0.8670747  0.19855933 0.21570878 0.98576554 0.99628926 0.13600705\n",
      " 0.21336265 0.36788706 0.21647892 0.08816527 0.46919351 0.99281689\n",
      " 0.93284148 0.06448175 0.22066778 0.42165776 0.96545431 0.20100289\n",
      " 0.33918184 0.05555032 0.05828571 0.06821441 0.99121735 0.18718325\n",
      " 0.99477917 0.31677733 0.87337178 0.2715355  0.82099456 0.14474486\n",
      " 0.4087899  0.17472343 0.2588072  0.09907613 0.2321126  0.87840242\n",
      " 0.06081879 0.96884903 0.35421778 0.23855695 0.14423517 0.96220386\n",
      " 0.17510551 0.11106755 0.17699808 0.90775525 0.07439    0.17494312\n",
      " 0.70391991 0.91031112 0.24370014 0.99666722 0.16428665 0.89004715\n",
      " 0.05810668 0.88405486 0.76932095 0.70420504 0.73256699 0.2962748\n",
      " 0.25731859 0.07943796 0.99921435 0.49274448 0.93077591 0.19251343\n",
      " 0.90887071 0.13796855 0.63446488 0.95775139 0.18425694 0.23969156\n",
      " 0.07855058 0.34751819 0.07639235 0.43624233 0.25692243 0.05650722\n",
      " 0.16395706 0.22013108 0.11607791 0.15181437 0.05550099 0.97829452\n",
      " 0.36114952 0.19657356 0.12583479 0.83230813 0.19062207 0.19816125\n",
      " 0.21798665 0.18435654 0.2507624  0.16681039 0.99257703 0.11830144\n",
      " 0.92161111 0.26118237 0.28824032 0.63316686 0.20982847 0.9599238\n",
      " 0.74258388 0.07962469 0.81699282 0.97370204 0.34195855 0.19064818\n",
      " 0.75654892 0.13264195 0.07156266 0.19400538 0.93929377 0.13766592\n",
      " 0.14769857 0.16234504 0.3885919  0.32369921 0.95857685 0.88576789\n",
      " 0.21538932 0.29382055 0.24724573 0.15565336 0.7012373  0.93164462\n",
      " 0.19771266 0.19625038 0.16751147 0.21927885 0.5638064  0.1284629\n",
      " 0.42165147 0.96642046 0.3545941  0.68975728 0.52674274 0.12990528\n",
      " 0.74309942 0.49521036 0.25933148 0.27038608 0.18908411 0.96157977\n",
      " 0.98542914 0.17887865 0.45202135 0.12569543 0.87322151 0.06565825\n",
      " 0.74257652 0.90708276 0.19339075 0.30453158 0.08668815 0.64324691\n",
      " 0.12597698 0.8661172  0.22226394 0.58196085 0.91531991 0.17999497\n",
      " 0.23550826 0.87314404 0.9992843  0.1366833  0.3150323  0.07422521\n",
      " 0.95489374 0.66985993 0.45124979 0.11776134 0.80862751 0.10535369\n",
      " 0.37282879 0.06722414 0.93206177 0.10642691 0.96404312 0.38146057\n",
      " 0.98253901 0.78518121 0.95713159 0.93442298 0.39356883 0.05729479\n",
      " 0.23939203 0.22628808 0.2704037  0.64404873 0.29791087 0.23369096\n",
      " 0.223169   0.09313525 0.06341106 0.5579344  0.07208987 0.13555314\n",
      " 0.88719415 0.13898088 0.32253527 0.9018666  0.88289243 0.9447466\n",
      " 0.86941691 0.1892328  0.25628742 0.98283502 0.12769692 0.15786741\n",
      " 0.88316392 0.59478909 0.63702222 0.38971357 0.2111575  0.14389017\n",
      " 0.26661061 0.18102932 0.86479847 0.9321909  0.99907726 0.31877303\n",
      " 0.81057215 0.99104633 0.34324228 0.18767258 0.16990155 0.24937533\n",
      " 0.19886729 0.31952894 0.81684632 0.04459891 0.22810341 0.28301275\n",
      " 0.42682457 0.08059269 0.3243648  0.97362333 0.91474587 0.14988989\n",
      " 0.96508262 0.32986656 0.9723399  0.51021021 0.19796219 0.66627674\n",
      " 0.0739473  0.25930936 0.11791945 0.96700625 0.68522288 0.94480075\n",
      " 0.24669877 0.03243834 0.99227666 0.84640911 0.06743407 0.44723994\n",
      " 0.58080743 0.24032572 0.69528225 0.73338114 0.10048555 0.48064856\n",
      " 0.07745044 0.14746072 0.11036229 0.25629127 0.17262128 0.19338378\n",
      " 0.06976219 0.99610508 0.85608077 0.36914074 0.12030799 0.11992757\n",
      " 0.93004958 0.03692464 0.18116101 0.21441053 0.15275625 0.95974944\n",
      " 0.15139171 0.91152912 0.66350707 0.93324753 0.2277163  0.99503076\n",
      " 0.37779813 0.77944292 0.17285776 0.97570873 0.99995902 0.16708242\n",
      " 0.1440726  0.12616916 0.20228599 0.12401267 0.13561546 0.26711259\n",
      " 0.06376418 0.99089858 0.98356137 0.83865199 0.68723408 0.94797881\n",
      " 0.47204456 0.27189797 0.26259073 0.12210833 0.14314296 0.21261134\n",
      " 0.19321338 0.92555329 0.2155467  0.6240026  0.26321637 0.44241479\n",
      " 0.45362072 0.44270748 0.26215416 0.99833031 0.73623847 0.73796381\n",
      " 0.13541124 0.94212526 0.21263312 0.09024457 0.54534948 0.99884305\n",
      " 0.94243852 0.68963257 0.47705419 0.80156902 0.81569026 0.94117569\n",
      " 0.11486534 0.99473426 0.26677075 0.89147391 0.08174882 0.23419463\n",
      " 0.88058724 0.59479312 0.64553663 0.93518966 0.12452636 0.08840929\n",
      " 0.18650099 0.3389918  0.91837266 0.23954829 0.26368241 0.99085241\n",
      " 0.40283669 0.44705809 0.47747169 0.8578524  0.59190692 0.35612728\n",
      " 0.29001998 0.22182603 0.96845288 0.49317376 0.05190301 0.92815372\n",
      " 0.20249626 0.24368543 0.05277743 0.4396766  0.33338216 0.88346342\n",
      " 0.57827763 0.96991754 0.11136092 0.02930808 0.31401805]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20316/798275831.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogreg_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlogreg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogreg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogreg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20316/4292640406.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Train adaline using batch gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[1;31m# Calculate the cost and gradient of cost with respect to weights and bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20316/4292640406.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\aipi540\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1009\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m         )\n\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\14183\\ProgramData\\Anaconda3\\envs\\aipi540\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1050\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "logreg_model = LogisticRegressionGD(eta=0.001,n_iter=100)\n",
    "logreg_model.fit(X_train_scaled,y_train)\n",
    "plt.plot(range(len(logreg_model.cost_path)),logreg_model.cost_path)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy is 0.956\n"
     ]
    }
   ],
   "source": [
    "test_preds = logreg_model.predict(X_test_scaled)\n",
    "test_preds = np.round(test_preds)\n",
    "test_acc = np.sum(test_preds==y_test)/len(y_test)\n",
    "print('Test set accuracy is {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Networks from Scratch\n",
    "### Binary Classification with NLL Loss and ReLu activation\n",
    "Shallow neural network for binary classification with one hidden layer and an output layer.  Network uses Negative Log Likelihood cost function and ReLu as the activation function of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_NLL_relu:\n",
    "    \n",
    "    def __init__(self,eta=0.05,n_iter=100,hidden_size=100,random_state=0):\n",
    "        self.eta=eta # Learning rate\n",
    "        self.n_iter=n_iter # Number of epochs\n",
    "        self.hidden_size=hidden_size # Number of units in the hidden layer\n",
    "        self.random_state=random_state\n",
    "    \n",
    "    def predict(self,X):\n",
    "        # Feed data through the neural network to get z scores of output\n",
    "        z_hidden = np.dot(X, self.W_1) + self.b_1\n",
    "        hiddenlayer_output = np.maximum(0, z_hidden) # linear combination plus relu\n",
    "        z_out = np.dot(hiddenlayer_output, self.W_out) + self.b_out # Output layer\n",
    "        yhat = 1. / (1. + np.exp(-z_out))\n",
    "        return yhat\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        epsilon = 1e-10\n",
    "        N = X.shape[0] # Number of observations\n",
    "        P = X.shape[1] # Number of features\n",
    "        \n",
    "        # Initialize the weights and biases to small random numbers\n",
    "        self.W_1 = 0.01 * np.random.randn(P,self.hidden_size) # Weights layer 1\n",
    "        self.b_1 = np.zeros((1,self.hidden_size)) # Biases layer 1\n",
    "        self.W_out = 0.01 * np.random.randn(self.hidden_size,1) # Weights in output layer\n",
    "        self.b_out = np.zeros((1,1)) # Bias in output layer\n",
    "        \n",
    "        # Train using batch gradient descent\n",
    "        for i in range(self.n_iter):\n",
    "            # Forward propagation to get the predictions\n",
    "            yhat = self.predict(X)\n",
    "            # Compute the average NLL loss\n",
    "            cost = 1/N*(-y.dot(np.log(yhat+epsilon)) - ((1-y).dot(np.log(1-yhat+epsilon))))\n",
    "            if i % 1000 == 0:\n",
    "                print(\"iteration {:d}: loss {:.5f}\".format(i, cost.item()))\n",
    "            \n",
    "            # Backpropagation\n",
    "\n",
    "            # Calculate the gradient of the average NLL loss with respect to output layer\n",
    "            gradient_z_out = -1/N*(y.reshape(-1,1)-yhat)\n",
    "            hiddenlayer_output = np.maximum(0, np.dot(X, self.W_1) + self.b_1) # linear combination plus relu\n",
    "            gradient_W_out = np.dot(hiddenlayer_output.T,gradient_z_out) # Gradient wrt output layer weights\n",
    "            gradient_b_out = np.sum(gradient_z_out) # Gradient wrt output layer bias\n",
    "            \n",
    "            # Backpropagate to hidden layer output\n",
    "            gradient_hidden_output = np.dot(gradient_z_out,self.W_out.T) # Hidden layer error\n",
    "            # Backpropagate back through relu to hidden layer z\n",
    "            grad_relu = np.ones(shape=gradient_hidden_output.shape) \n",
    "            grad_relu[hiddenlayer_output <=0]=0  # Gradient of hidden layer output wrt ReLu\n",
    "            gradient_z_hidden = gradient_hidden_output * grad_relu # Gradient of hidden layer wrt z_hidden\n",
    "            # Backpropagate to W_1 and b_1\n",
    "            gradient_W_1 = np.dot(X.T,gradient_z_hidden) # Gradient wrt weights layer 1\n",
    "            gradient_b_1 = np.sum(gradient_z_hidden,axis=0) # Gradient wrt biases layer 1\n",
    "            \n",
    "            # Update the weights and biases\n",
    "            self.W_1 -= self.eta * gradient_W_1\n",
    "            self.b_1 -= self.eta * gradient_b_1\n",
    "            self.W_out -= self.eta * gradient_W_out\n",
    "            self.b_out -= self.eta * gradient_b_out\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.69296\n",
      "iteration 1000: loss 0.05362\n",
      "iteration 2000: loss 0.04102\n",
      "iteration 3000: loss 0.03294\n",
      "iteration 4000: loss 0.02720\n",
      "iteration 5000: loss 0.02270\n",
      "iteration 6000: loss 0.01916\n",
      "iteration 7000: loss 0.01636\n",
      "iteration 8000: loss 0.01409\n",
      "iteration 9000: loss 0.01213\n",
      "Test set accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "nn_model = NeuralNet_NLL_relu(eta=0.05,n_iter=10000)\n",
    "nn_model.fit(X_train_scaled,y_train)\n",
    "\n",
    "# evaluate test set accuracy\n",
    "yhat = nn_model.predict(X_test_scaled).squeeze()\n",
    "yhat = (yhat>=0.5).astype(int)\n",
    "acc = np.sum(yhat.reshape(-1)==y_test)/len(y_test)\n",
    "\n",
    "print('Test set accuracy: {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification with NLL Loss and sigmoid activation\n",
    "Shallow neural network for binary classification with one hidden layer and an output layer.  Network uses Negative Log Likelihood cost function and sigmoid function as the activation function of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_NLL_sigmoid:\n",
    "    \n",
    "    def __init__(self,eta=0.05,n_iter=100,hidden_size=100,random_state=0):\n",
    "        self.eta=eta # Learning rate\n",
    "        self.n_iter=n_iter # Number of epochs\n",
    "        self.hidden_size=hidden_size # Number of units in the hidden layer\n",
    "        self.random_state=random_state\n",
    "    \n",
    "    def predict(self,X):\n",
    "        ### BEGIN SOLUTION ###\n",
    "        # Feed data through the neural network to get z scores of output\n",
    "        z_hidden = np.dot(X, self.W_1) + self.b_1\n",
    "        hiddenlayer_output = 1. / (1. + np.exp(-z_hidden))\n",
    "        z_out = np.dot(hiddenlayer_output, self.W_out) + self.b_out # Output layer\n",
    "        yhat = 1. / (1. + np.exp(-z_out))\n",
    "        ### END SOLUTION ###\n",
    "        return yhat\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        epsilon = 1e-10\n",
    "        N = X.shape[0] # Number of observations\n",
    "        P = X.shape[1] # Number of features\n",
    "        \n",
    "        # Initialize the weights and biases to small random numbers\n",
    "        self.W_1 = 0.01 * np.random.randn(P,self.hidden_size) # Weights layer 1\n",
    "        self.b_1 = np.zeros((1,self.hidden_size)) # Biases layer 1\n",
    "        self.W_out = 0.01 * np.random.randn(self.hidden_size,1) # Weights in output layer\n",
    "        self.b_out = np.zeros((1,1)) #Bias in output layer\n",
    "        \n",
    "        # Train using batch gradient descent\n",
    "        for i in range(self.n_iter):\n",
    "            ### BEGIN SOLUTION ###\n",
    "            \n",
    "            # Forward propagation to get the predictions\n",
    "            yhat = self.predict(X)\n",
    "            \n",
    "            # Compute the average NLL loss\n",
    "            cost = 1/N*(-y.dot(np.log(yhat+epsilon)) - ((1-y).dot(np.log(1-yhat+epsilon))))\n",
    "            if i % 1000 == 0:\n",
    "                print(\"iteration {:d}: loss {:.5f}\".format(i, cost.item()))\n",
    "            \n",
    "            # Backpropagation\n",
    "            # Calculate the gradient of the average NLL loss with respect to output layer weights and bias\n",
    "            gradient_z_out = -1/N*(y.reshape(-1,1)-yhat)\n",
    "            hiddenlayer_output = 1. / (1. + np.exp(-(np.dot(X, self.W_1) + self.b_1)))\n",
    "            gradient_W_out = np.dot(hiddenlayer_output.T,gradient_z_out)\n",
    "            gradient_b_out = np.sum(gradient_z_out)\n",
    "            \n",
    "            # Backpropagate to hidden layer output\n",
    "            gradient_hidden_output = np.dot(gradient_z_out,self.W_out.T)\n",
    "            # Backpropagate through sigmoid to hidden layer z\n",
    "            gradient_z_hidden = gradient_hidden_output * (hiddenlayer_output * (1-hiddenlayer_output))\n",
    "            # Backpropagate to W_1 and b_1\n",
    "            gradient_W_1 = np.dot(X.T,gradient_z_hidden) # Gradient of cost wrt weights layer 1\n",
    "            gradient_b_1 = np.sum(gradient_z_hidden,axis=0) # Gradient of cost wrt biases layer 1\n",
    "            \n",
    "            # Update the weights and biases\n",
    "            self.W_1 -= self.eta * gradient_W_1\n",
    "            self.b_1 -= self.eta * gradient_b_1\n",
    "            self.W_out -= self.eta * gradient_W_out\n",
    "            self.b_out -= self.eta * gradient_b_out\n",
    "        \n",
    "        ### END SOLUTION ###\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.68694\n",
      "iteration 1000: loss 0.05184\n",
      "iteration 2000: loss 0.04564\n",
      "iteration 3000: loss 0.04211\n",
      "iteration 4000: loss 0.03927\n",
      "iteration 5000: loss 0.03626\n",
      "iteration 6000: loss 0.03263\n",
      "iteration 7000: loss 0.02842\n",
      "iteration 8000: loss 0.02422\n",
      "iteration 9000: loss 0.02043\n",
      "Test set accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "nn_model = NeuralNet_NLL_sigmoid(eta=0.3,n_iter=10000)\n",
    "nn_model.fit(X_train_scaled,y_train)\n",
    "\n",
    "# evaluate test set accuracy\n",
    "yhat = nn_model.predict(X_test_scaled).squeeze()\n",
    "yhat = (yhat>=0.5).astype(int)\n",
    "acc = np.sum(yhat.reshape(-1)==y_test)/len(y_test)\n",
    "\n",
    "print('Test set accuracy: {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification with Cross Entropy Loss\n",
    "Even though we only have two classes, we can implement a neural network for multiclass classificaiton using a softmax layer to get the output probabilities.  Since we use a softmax rather than a sigmoid (as we usually do in binary classification), the below implementation uses cross entropy as the loss function (rather than NLL).  An explanation of the math behind the cross entropy loss function and its gradient can be found [here](http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.69400\n",
      "iteration 1000: loss 0.04781\n",
      "iteration 2000: loss 0.03386\n",
      "iteration 3000: loss 0.02544\n",
      "iteration 4000: loss 0.01966\n",
      "iteration 5000: loss 0.01547\n",
      "iteration 6000: loss 0.01234\n",
      "iteration 7000: loss 0.00998\n",
      "iteration 8000: loss 0.00818\n",
      "iteration 9000: loss 0.00683\n",
      "iteration 10000: loss 0.00578\n",
      "iteration 11000: loss 0.00494\n",
      "iteration 12000: loss 0.00427\n",
      "iteration 13000: loss 0.00373\n",
      "iteration 14000: loss 0.00328\n",
      "iteration 15000: loss 0.00291\n",
      "iteration 16000: loss 0.00261\n",
      "iteration 17000: loss 0.00235\n",
      "iteration 18000: loss 0.00213\n",
      "iteration 19000: loss 0.00194\n",
      "Test set accuracy: 0.974\n"
     ]
    }
   ],
   "source": [
    "class NeuralNet_CrossEntropy:\n",
    "    \n",
    "    def __init__(self,eta=0.01,n_iter=100,hidden_size=100,random_state=0):\n",
    "        self.eta=eta # Learning rate\n",
    "        self.n_iter=n_iter # Number of epochs\n",
    "        self.hidden_size=hidden_size # Number of units in the hidden layer\n",
    "        self.random_state=random_state\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        N = X.shape[0] # Number of observations\n",
    "        P = X.shape[1] # Number of features\n",
    "        out_classes = len(np.unique(y)) # Number of classes in y\n",
    "        \n",
    "        # Initialize the weights to small random numbers\n",
    "        self.W1 = 0.01 * np.random.randn(P,self.hidden_size)\n",
    "        self.b1 = np.zeros((1,self.hidden_size))\n",
    "        self.W2 = 0.01 * np.random.randn(self.hidden_size,out_classes)\n",
    "        self.b2 = np.zeros((1,out_classes))\n",
    "        \n",
    "        # Train using batch gradient descent\n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            # Get the predictions\n",
    "            yhat = self.predict(X)\n",
    "            \n",
    "            # Compute the average cross-entropy loss\n",
    "            # Avg cross-entropy loss: 1/N * (-log(probability of correct class (y)))\n",
    "            correct_logprobs = -np.log(yhat[range(N),y])\n",
    "            loss = 1/N*np.sum(correct_logprobs)\n",
    "            if i % 1000 == 0:\n",
    "                print(\"iteration {:d}: loss {:.5f}\".format(i, loss))\n",
    "            \n",
    "            # Calculate the gradient of the average cross-entropy loss with respect to z_out\n",
    "            # Gradient = 1/size * (yhat - p(x)), where p(x) is the one-hot encoded vector of y\n",
    "            # E.g. if y = 0 and 3 classes, p(x)=[1,0,0]. If y=2, p(x) = [0,0,1]\n",
    "            gradient = yhat.copy()\n",
    "            gradient[range(N),y] = gradient[range(N),y] - 1\n",
    "            gradient *= 1/N # Get the average over all observations in batch\n",
    "            \n",
    "            # Backpropate the gradient to the parameters\n",
    "            # First backprop to get gradient with respect to W2 and b2 (weights between hidden layer and output)\n",
    "            hiddenlayer_output = np.maximum(0, np.dot(X, self.W1) + self.b1) # linear combination plus relu\n",
    "            dW2 = np.dot(hiddenlayer_output.T, gradient)\n",
    "            db2 = np.sum(gradient, axis=0, keepdims=True)\n",
    "            \n",
    "            # Calculate hidden layer error term\n",
    "            hidden_error = np.dot(gradient, self.W2.T)\n",
    "            # Backpropagate the ReLU non-linearity\n",
    "            hidden_error[hiddenlayer_output <= 0] = 0\n",
    "            # Backprop to get gradient wrt W1 and b1\n",
    "            dW1 = np.dot(X.T, hidden_error)\n",
    "            db1 = np.sum(hidden_error, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update the weights and biases\n",
    "            self.W1 += -self.eta * dW1\n",
    "            self.b1 += -self.eta * db1\n",
    "            self.W2 += -self.eta * dW2\n",
    "            self.b2 += -self.eta * db2\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        # Feed data through the neural network to get z scores of output\n",
    "        hiddenlayer_output = np.maximum(0, np.dot(X, self.W1) + self.b1) # First hidden layer incl. relu\n",
    "        z_out = np.dot(hiddenlayer_output, self.W2) + self.b2 # Output layer\n",
    "        # Compute the class probabilities from the z scores using softmax function\n",
    "        yhat = np.exp(z_out) / np.sum(np.exp(z_out), axis=1, keepdims=True) # Shape: (num_examples,num_classes)\n",
    "        return yhat\n",
    "\n",
    "\n",
    "nn_model = NeuralNet_CrossEntropy(eta=0.05,n_iter=20000)\n",
    "nn_model.fit(X_train_scaled,y_train)\n",
    "\n",
    "# evaluate test set accuracy\n",
    "yhat = nn_model.predict(X_test_scaled)\n",
    "predicted_class = np.argmax(yhat, axis=1)\n",
    "print('Test set accuracy: {:.3f}'.format(np.mean(predicted_class == y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification with Cross Entropy Loss and L2 Regularization\n",
    "Implementation of the above network which includes L2 regularization together with cross entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.69106\n",
      "iteration 1000: loss 0.04660\n",
      "iteration 2000: loss 0.03258\n",
      "iteration 3000: loss 0.02440\n",
      "iteration 4000: loss 0.01890\n",
      "iteration 5000: loss 0.01490\n",
      "iteration 6000: loss 0.01202\n",
      "iteration 7000: loss 0.00986\n",
      "iteration 8000: loss 0.00822\n",
      "iteration 9000: loss 0.00695\n",
      "iteration 10000: loss 0.00594\n",
      "iteration 11000: loss 0.00513\n",
      "iteration 12000: loss 0.00447\n",
      "iteration 13000: loss 0.00392\n",
      "iteration 14000: loss 0.00347\n",
      "iteration 15000: loss 0.00309\n",
      "iteration 16000: loss 0.00277\n",
      "iteration 17000: loss 0.00250\n",
      "iteration 18000: loss 0.00227\n",
      "iteration 19000: loss 0.00208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetwithRegularization at 0x7fb9eee51e90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwithRegularization:\n",
    "    \n",
    "    def __init__(self,eta=0.01,n_iter=100,hidden_size=100,reg=1e-3,random_state=0):\n",
    "        self.eta=eta # Learning rate\n",
    "        self.n_iter=n_iter # Number of epochs\n",
    "        self.hidden_size=hidden_size # Number of units in the hidden layer\n",
    "        self.reg=1e-3 # Regularization penalty\n",
    "        self.random_state=random_state\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        N = X.shape[0] # Number of observations\n",
    "        P = X.shape[1] # Number of features\n",
    "        out_classes = len(np.unique(y)) # Number of classes in y\n",
    "        \n",
    "        # Initialize the weights to small random numbers\n",
    "        self.W1 = 0.01 * np.random.randn(P,self.hidden_size)\n",
    "        self.b1 = np.zeros((1,self.hidden_size))\n",
    "        self.W2 = 0.01 * np.random.randn(self.hidden_size,out_classes)\n",
    "        self.b2 = np.zeros((1,out_classes))\n",
    "        \n",
    "        # Train using batch gradient descent\n",
    "        for i in range(self.n_iter):\n",
    "            \n",
    "            # Get the predictions\n",
    "            yhat = self.predict(X)\n",
    "            \n",
    "            # Compute the total average loss = 1/N * (cross-entropy loss + regularization loss)\n",
    "            # Cross-entropy loss: (-log(normalized probability of correct class))\n",
    "            # Regularization loss: 0.5 * reg * sum of weights squared, across all features and classes\n",
    "            correct_logprobs = -np.log(yhat[range(N),y])\n",
    "            data_loss = np.sum(correct_logprobs)\n",
    "            reg_loss = 0.5*self.reg*np.sum(self.W1*self.W1) + 0.5*self.reg*np.sum(self.W2*self.W2)\n",
    "            loss = 1/N * (data_loss + reg_loss)\n",
    "            if i % 1000 == 0:\n",
    "                print(\"iteration {:d}: loss {:.5f}\".format(i, loss))\n",
    "            \n",
    "            # Calculate the gradient of the average cross-entropy loss\n",
    "            # Gradient = 1/size * (yhat - p(x)), where p(x) is the one-hot encoded vector of y\n",
    "            # E.g. if y = 0 and 3 classes, p(x)=[1,0,0]. If y=2, p(x) = [0,0,1]\n",
    "            gradient = yhat.copy()\n",
    "            gradient[range(N),y] = gradient[range(N),y] - 1\n",
    "            gradient *= 1/N # Get the average over all observations in batch\n",
    "            \n",
    "            # Backpropate the gradient to the parameters\n",
    "            # First backprop to get gradient wrt W2 and b2\n",
    "            hidden_layer = np.maximum(0, np.dot(X, self.W1) + self.b1) # linear combination plus relu\n",
    "            dW2 = np.dot(hidden_layer.T, gradient)\n",
    "            db2 = np.sum(gradient, axis=0, keepdims=True)\n",
    "            \n",
    "            # Calculate hidden layer error term\n",
    "            hidden_error = np.dot(gradient, self.W2.T)\n",
    "            # Backpropagate the ReLU non-linearity\n",
    "            hidden_error[hidden_layer <= 0] = 0\n",
    "            # Backprop to get gradient wrt W1 and b1\n",
    "            dW1 = np.dot(X.T, hidden_error)\n",
    "            db1 = np.sum(hidden_error, axis=0, keepdims=True)\n",
    "            \n",
    "            # Add regularization gradient contribution to the gradients\n",
    "            dW2 += 1/N * (self.reg * self.W2)\n",
    "            dW1 += 1/N * (self.reg * self.W1)\n",
    "            \n",
    "            # Update the weights and biases\n",
    "            self.W1 += -self.eta * dW1\n",
    "            self.b1 += -self.eta * db1\n",
    "            self.W2 += -self.eta * dW2\n",
    "            self.b2 += -self.eta * db2\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        # Feed data through the neural network to get z scores of output\n",
    "        hidden_layer = np.maximum(0, np.dot(X, self.W1) + self.b1) # First hidden layer incl. relu\n",
    "        out_scores = np.dot(hidden_layer, self.W2) + self.b2 # Output layer\n",
    "        # Compute the class probabilities from the z scores using softmax function\n",
    "        yhat = np.exp(out_scores) / np.sum(np.exp(out_scores), axis=1, keepdims=True) # Shape: (num_examples,num_classes)\n",
    "        return yhat\n",
    "\n",
    "\n",
    "nn_model = NeuralNetwithRegularization(eta=0.05,reg=1e-3, n_iter=20000)\n",
    "nn_model.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "# evaluate test set accuracy\n",
    "yhat = nn_model.predict(X_test_scaled)\n",
    "predicted_class = np.argmax(yhat, axis=1)\n",
    "print('Test accuracy: {:.3f}'.format(np.mean(predicted_class == y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
