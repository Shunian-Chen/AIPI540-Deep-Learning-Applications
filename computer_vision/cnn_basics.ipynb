{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OkAhQG2V8n"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3bKXytY2V8r"
      },
      "source": [
        "# PyTorch CNN Basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZRokJihAd3v"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if working in Colab\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "git_user = \"AIPI540\" # Enter user or organization name\n",
        "git_email = \"jon.reifschneider@gmail.com\" # Enter your email\n",
        "repo_name = \"class_demos\" # Enter repo name\n",
        "# Use the below if repo is private, or is public and you want to push to it\n",
        "# Otherwise comment next two lines out\n",
        "#git_token = input(\"enter git token\") # Enter your github token \n",
        "#git_path = f\"https://{git_token}@github.com/{git_user}/{repo_name}.git\"\n",
        "# Use the below instead if repo is public and you do not need to push to it\n",
        "git_path = 'https://github.com/AIPI540/class_demos.git'\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = 'computer_vision'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls\n",
        "\n",
        "# Mount Google drive to access files there\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# # Path to files in Google Drive\n",
        "# path_to_gdrive_files = '../../gdrive/MyDrive/AIPI540/class_demos/computer_vision'\n",
        "# # Display contents\n",
        "# print('Contents of gdrive directory:')\n",
        "# print(os.listdir(path_to_gdrive_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77g7pHdf2V8s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDFQ76xp2V8t",
        "outputId": "82815432-b665-45ba-e398-f055a7405ee4"
      },
      "outputs": [],
      "source": [
        "# Load training and test data\n",
        "\n",
        "# Set up transforms - ToTensor only in this case\n",
        "data_transform = transforms.Compose([transforms.ToTensor()]) # Converts pixel values to tensor in range [0.,1.]\n",
        "\n",
        "# Set up datasets\n",
        "train_data = FashionMNIST(root='data', train=True,\n",
        "                                   download=True, transform=data_transform)\n",
        "\n",
        "test_data = FashionMNIST(root='data', train=False,\n",
        "                                  download=True, transform=data_transform)\n",
        "\n",
        "print('Training data images: ', len(train_data))\n",
        "print('Test data images: ', len(test_data))\n",
        "\n",
        "# Specify the image classes\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MADTyn4t2V8u"
      },
      "source": [
        "### Step 1: Set up dataloaders for our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f89T3iTl2V8v"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "mooaLTX_2V8v",
        "outputId": "76b7aed3-31a4-4208-a755-94902d15f2a4"
      },
      "outputs": [],
      "source": [
        "# Get next batch of training images\n",
        "images, labels = iter(train_loader).next()\n",
        "print(images.shape)\n",
        "images = images.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "for idx in range(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(classes[labels[idx]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TQcrPtA2V8w"
      },
      "source": [
        "As we see above, the inputs in a PyTorch DataLoader are of shape [N,C,H,W) where:  \n",
        "- N = batch size  \n",
        "- C = number of channels (1 for grayscale, 3 for RGB color)  \n",
        "- H = image height  \n",
        "- W = image width"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3EGj5MS2V8w"
      },
      "source": [
        "### Step 2: Define our neural network architecture\n",
        "\n",
        "To define a neural network in PyTorch, you define the layers of a model in the function `__init__` and define the feedforward behavior of the network in the function `forward`, which takes in an input image tensor, `x`. During training, PyTorch will perform backpropagation by keeping track of the network's feedforward behavior and using autograd to calculate the update to the weights in the network.\n",
        "\n",
        "A conv/pool layer may be defined like this (in `__init__`):\n",
        "```\n",
        "# Convolutional layer with 1 input channel (grayscale), 32 output channels/feature maps and a 3x3 square convolution kernel\n",
        "self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "\n",
        "# maxpool that uses a square window of kernel_size=2, stride=2\n",
        "self.pool = nn.MaxPool2d(2, 2)      \n",
        "```\n",
        "\n",
        "We then define the `forward` function which governs the flow of input data through the neural network.  In this case we will apply a ReLu activation function after the convolutional layers followed by a MaxPooling layer:\n",
        "```\n",
        "x = self.pool(F.relu(self.conv1(x)))\n",
        "```\n",
        "\n",
        "Any layers with trainable weights, such as convolutional layers, must be contained in the `__init__` function. Any layers or functions that always behave in the same way, such as a pre-defined activation function, may appear in the `forward` function without being defined in the `__init__` function. In practice, convolutional and pooling layers are defined in `__init__` and activations are usually defined in `forward`.\n",
        "\n",
        "For convolutional layers, the output feature maps will have the specified depth and the dimensions of each output feature map (width/height) can be computed as the input image width/height (W or H) minus the filter size (F) divided by the stride (S) + 1. The equation to calculate the output dimensions of the convolution layer is: `output_dim = (W-F)/S + 1`, for a padding size of 0. A derivation of this formula can be found [here](http://cs231n.github.io/convolutional-networks/#conv).\n",
        "\n",
        "For a pool layer, the output can be computed as the input width/height (W) minus the filter size (F) divided by the stride (S) + 1.  The equation to calculate the output shape of the pooling layer is: `output_dim = (W-F)/S + 1`\n",
        "\n",
        "To feed the output of a convolutional/pooling layer into a linear layer, we must first flatten the extracted features into a vector. In PyTorch we can flatten an input `x` by using `x = x.view(x.size(0), -1)`.\n",
        "\n",
        "Finally, depending on our task we apply a final activation layer (e.g. softmax) to convert the outputs from the linear layer into class scores.  Note that some loss functions (e.g. CrossEntropy loss) include the softmax so we do not need to define it separately in the neural network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3yfba9A2V8x"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        '''\n",
        "        Input shape: (batch_size,1,28,28)\n",
        "        \n",
        "        Convolutional 1 layer: 3x3 kernel, stride=1, 10 output channels / feature maps\n",
        "        Conv1 layer output dimension = (W-F)/S+1 = (28-3)/1+1 = 26\n",
        "        Conv1 layer output shape for one image: (10,26,26)\n",
        "        \n",
        "        Pooling layer: kernel_size=2,stride=2\n",
        "        Pooling1 layer output dimension = (W-F)/S+1 = (26-2)/2+1 = 13\n",
        "        Pooling1 output shape for one image: (10,13,13)\n",
        "        \n",
        "        Convolutional 2 layer: 3x3 kernel, stride=1, 20 output channels / feature maps\n",
        "        Conv2 layer output dimension = (W-F)/S+1 = (13-3)/1+1 = 11\n",
        "        Conv2 layer output shape for one image: (20,11,11)\n",
        "        \n",
        "        Pooling layer: kernel_size=2,stride=2\n",
        "        Pooling2 layer output dimension = (W-F)/S+1 = (11-2)/2+1 = 5.5 <- round down from 5.5 to 5\n",
        "        Pooling2 output shape for one image: (20,5,5)\n",
        "        \n",
        "        \n",
        "        Linear layer input shape: (batch_size, 20 channels * 5 * 5)\n",
        "        Linear layer output shape: (batch_size, 10 output classes)\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        # Convolutional 1 layer: 3x3 kernel, stride=1, 10 output channels / feature maps\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1)\n",
        "        # Conv1 layer output size = (W-F)/S+1 = (28-3)/1+1 = 26\n",
        "        # Conv1 layer output shape for one image: (10,26,26)\n",
        "        \n",
        "        # maxpool layer\n",
        "        # pool with kernel_size=2, stride=2\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Pool output shape for one image: (10,13,13)\n",
        "        \n",
        "        # Convolutional 2 layer: 3x3 kernel, stride=1, 20 output channels / feature maps\n",
        "        self.conv2 = nn.Conv2d(in_channels=10,out_channels=20,kernel_size=3)\n",
        "        # Conv2 layer output size = (W-F)/S+1 = (13-3)/1+1 = 11\n",
        "        # Conv2 layer output shape for one image: (20,11,11)\n",
        "        \n",
        "        # maxpool layer\n",
        "        # pool with kernel_size=2, stride=2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Pool output shape for one image: (20,5,5)\n",
        "        \n",
        "        # Input size: 20 * 5 * 5 = 500 from pool2 pooling layer\n",
        "        # 10 output channels (for the 10 classes)\n",
        "        self.fc1 = nn.Linear(20*5*5, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Two convolutional layers followed by relu and then pooling\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "\n",
        "        # Flatten into a vector to feed into linear layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Linear layer\n",
        "        x = self.fc1(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "net = ConvNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUQscTuH2V8x"
      },
      "source": [
        "### Step 3: Define a cost / loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q1hclmU2V8y"
      },
      "outputs": [],
      "source": [
        "# Cross entropy loss combines softmax and nn.NLLLoss() in one single class.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ARCSO3Z2V8z"
      },
      "source": [
        "### Step 4: Train the model\n",
        "To train our model, we perform the following four steps in a loop, using one input mini-batch at a time:  \n",
        "    1) Make a forward pass through the network to calculate the network output  \n",
        "    2) Use the network output to calculate the cost/loss  \n",
        "    3) Calculate the gradient of the cost/loss with respect to the weights by performing a backward pass through the network with loss.backward()  \n",
        "    4) Update the weights by taking a step with the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVHyrYa82V8z"
      },
      "source": [
        "If GPU is available, we can move the model (all its parameter Tensors) onto GPU. If we decided to use GPU, we also need to put all the data Tensors used in training onto GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuLOntO_2V8z"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88tHH9ND2V80"
      },
      "outputs": [],
      "source": [
        "def train_model(model,criterion,optimizer,train_loader,n_epochs,device):\n",
        "    \n",
        "    loss_over_time = [] # to track the loss as the network trains\n",
        "    \n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    model.train() # Set the model to training mode\n",
        "    \n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        \n",
        "        for i, data in enumerate(train_loader):\n",
        "            \n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Zero the weight gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass to get outputs\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backpropagation to get the gradients with respect to each weight\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Convert loss into a scalar and add it to running_loss\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            if i % 1000 == 999:    # print every 1000 batches\n",
        "                avg_loss = running_loss/1000\n",
        "                # record and print the avg loss over the 1000 batches\n",
        "                loss_over_time.append(avg_loss)\n",
        "                print('Epoch: {}, Batch: {}, Avg. Loss: {:.4f}'.format(epoch + 1, i+1, avg_loss))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    return loss_over_time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "KRDVI8xZ2V81",
        "outputId": "43548475-ab2f-422b-c491-27387afe79f8"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "n_epochs = 5\n",
        "cost_path = train_model(net,criterion,optimizer,train_loader,n_epochs,device)\n",
        "\n",
        "# visualize the loss as the network trained\n",
        "plt.plot(cost_path)\n",
        "plt.xlabel('Batch (1000s)')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwUvEmWs2V81"
      },
      "source": [
        "### Step 5: Test the model on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Oq4PTTR2V82"
      },
      "source": [
        "`MODEL.train()` and `MODEL.eval()` are functions acting as a switch which turns on/off any randomness of layers in the model such as dropout which are used to improve training but which we do not want to use while testing / in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5yUE9412V82"
      },
      "outputs": [],
      "source": [
        "def test_model(model,test_loader,device):\n",
        "    # Turn autograd off\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # Set up lists to store true and predicted values\n",
        "        y_true = []\n",
        "        test_preds = []\n",
        "\n",
        "        # Calculate the predictions on the test set and add to list\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # Feed inputs through model to get raw scores\n",
        "            logits = model.forward(inputs)\n",
        "            # Convert raw scores to probabilities (not necessary since we just care about discrete probs in this case)\n",
        "            probs = F.softmax(logits,dim=1)\n",
        "            # Get discrete predictions using argmax\n",
        "            preds = np.argmax(probs.cpu().numpy(),axis=1)\n",
        "            # Add predictions and actuals to lists\n",
        "            test_preds.extend(preds)\n",
        "            y_true.extend(labels)\n",
        "\n",
        "        # Calculate the accuracy\n",
        "        test_preds = np.array(test_preds)\n",
        "        y_true = np.array(y_true)\n",
        "        test_acc = np.sum(test_preds == y_true)/y_true.shape[0]\n",
        "        \n",
        "        # Recall for each class\n",
        "        recall_vals = []\n",
        "        for i in range(10):\n",
        "            class_idx = np.argwhere(y_true==i)\n",
        "            total = len(class_idx)\n",
        "            correct = np.sum(test_preds[class_idx]==i)\n",
        "            recall = correct / total\n",
        "            recall_vals.append(recall)\n",
        "    \n",
        "    return test_acc,recall_vals\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDQMTjl4aAwz",
        "outputId": "e8fdf78b-91ed-45c8-a0ad-794d9b212bb0"
      },
      "outputs": [],
      "source": [
        "acc,recall_vals = test_model(net,test_loader,device)\n",
        "print('Test set accuracy is {:.3f}'.format(acc))\n",
        "for i in range(10):\n",
        "    print('For class {}, recall is {}'.format(classes[i],recall_vals[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "9bGA5Jgx2V83",
        "outputId": "5eef2a8b-dcf8-4c48-ce5b-6eb197072bde"
      },
      "outputs": [],
      "source": [
        "# Get a batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "# get predictions\n",
        "preds = np.squeeze(net(images).max(1, keepdim=True)[1].cpu().numpy())\n",
        "images = images.cpu().numpy()\n",
        "\n",
        "# Plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWkCD9MP2V83"
      },
      "source": [
        "### Saving models\n",
        "To save PyTorch models for later use, we have two options:  \n",
        "1) We can save the `state_dict` which contains all the learned parameters of the model (the weights and biases) but not the architecture itself.  To use it, we instantiate a new model of the desired architecture and then load the saved `state_dict` to assign values to all the parameters in the model  \n",
        "2) We can alternatively save the entire model including the architecture, and then load it up and use it for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfcetXJt2V84"
      },
      "outputs": [],
      "source": [
        "# OPTION 1: Save the state dictionary of the model\n",
        "import os\n",
        "model_dir = 'saved_models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "filename = 'model_state_dict.pt'\n",
        "\n",
        "# Save the model's learned parameters (state_dict)\n",
        "torch.save(net.state_dict(), os.path.join(model_dir,filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzQ1LGul2V84",
        "outputId": "18f1f479-5e79-4759-aac6-849190f733b5"
      },
      "outputs": [],
      "source": [
        "# Initialize new model as ConvNet() class and load state dict previously saved\n",
        "model = ConvNet()\n",
        "model.load_state_dict(torch.load(os.path.join(model_dir,filename)))\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "    \n",
        "# Test loaded model\n",
        "acc,recall_vals = test_model(model,test_loader,device)\n",
        "print()\n",
        "print('Test set accuracy is {:.3f}'.format(acc))\n",
        "for i in range(10):\n",
        "    print('For class {}, recall is {}'.format(classes[i],recall_vals[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsgpJ2Aw2V84"
      },
      "outputs": [],
      "source": [
        "# OPTION 2: Save the entire model\n",
        "\n",
        "model_dir = 'saved_models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "filename = 'fullmodel.pt'\n",
        "\n",
        "# Save the entire model\n",
        "torch.save(net, os.path.join(model_dir,filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b4X29zS2V85",
        "outputId": "499b16af-bb4e-4e9b-edcd-6cf4a9d49125"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "model2 = torch.load(os.path.join(model_dir,filename))\n",
        "\n",
        "# Test loaded model\n",
        "acc,recall_vals = test_model(model2,test_loader,device)\n",
        "print('Test set accuracy is {:.3f}'.format(acc))\n",
        "for i in range(10):\n",
        "    print('For class {}, recall is {}'.format(classes[i],recall_vals[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd6d5Y8L2V85"
      },
      "source": [
        "# Using Pre-trained Models\n",
        "Rather than creating our own model from scratch, we can use models which have been pre-trained on other generic tasks to take advantage of the training that has already occurred.  When we do this, we usually keep the model parameters fixed except for the last layer.  We use a new last layer (a fully connected Linear layer) which we then train on our specific task.\n",
        "\n",
        "All models which have been pre-trained on ImageNet expect input images normalized in the same way, i.e. mini-batches of 3-channel\n",
        "RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded with pixel values a range of [0, 1] (accomplished by using `transforms.ToTensor()`) and then normalized using the mean and standard deviation of image pixel values.  Since it can be difficult to calculate these statistics, we often just normalize by the mean and standard deviation of the ImageNet images, which are mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. You can use the following transform to normalize: `normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])`\n",
        "\n",
        "In this example we will load a ResNet18 model and use it on the FashionMNIST dataset. The FashionMNIST dataset has a couple of differences relative to the ImageNet dataset on which pretrained models are trained:  \n",
        "- It is grayscale so has only 1 channel instead of 3. \n",
        "- It is 28*28 rather than 224*224. \n",
        "- The pixel values mean and standard deviation are quite different from the images in ImageNet  \n",
        "- We have only 10 classes as output\n",
        "\n",
        "Because of those differences, we need to make two adjustments to use our pretrained model:  \n",
        "1) We need to transform our input image data to be 224*224 and normalize using the images mean and standard deviation  \n",
        "2) We need to change our net architecture to accomodate 1 input channel, and 10 output classes\n",
        "\n",
        "NOTE: THIS MUST BE RUN ON GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8T4KRUO2V86"
      },
      "outputs": [],
      "source": [
        "# We need to add a transformation to our input image data\n",
        "# Resnet18 expects input images of shape 224*224\n",
        "# We also should normalize our pixel values by subracting mean and dividing by standard deviation of training set values\n",
        "\n",
        "train_data = FashionMNIST(root='data', train=True, download=True)\n",
        "\n",
        "data_transform = transforms.Compose([ transforms.Resize((224, 224)),\n",
        "                                         transforms.ToTensor(), \n",
        "                                         transforms.Normalize((train_data.data.float().mean()/255), (train_data.data.float().std()/255))])\n",
        "\n",
        "train_data_resnet = FashionMNIST(root='data', train=True,\n",
        "                                   download=True, transform=data_transform)\n",
        "\n",
        "test_data_resnet = FashionMNIST(root='data', train=False,\n",
        "                                  download=True, transform=data_transform)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader_resnet = DataLoader(train_data_resnet,batch_size=batch_size, shuffle=True)\n",
        "test_loader_resnet = DataLoader(test_data_resnet,batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Specify the image classes\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "1b-JfYbp2V86",
        "outputId": "c48a5cc1-1124-4539-d6a2-787bbe1863a2"
      },
      "outputs": [],
      "source": [
        "# Load a resnet18 pre-trained model\n",
        "model_resnet = torchvision.models.resnet18(pretrained=True)\n",
        "# Shut off autograd for all layers to freeze model so the layer weights are not trained\n",
        "for param in model_resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# Replace the resnet input layer to take in grayscale images (1 input channel), since it was trained on color (3 input channels)\n",
        "in_channels = 1\n",
        "model_resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Replace the resnet final layer with a new fully connected Linear layer we will train on our task\n",
        "# Number of out units is number of classes (10)\n",
        "num_ftrs = model_resnet.fc.in_features\n",
        "model_resnet.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_resnet.parameters(), lr=0.001)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cost_path = train_model(model_resnet,criterion,optimizer,train_loader_resnet,n_epochs,device)\n",
        "\n",
        "# Visualize the loss as the network trained\n",
        "plt.plot(cost_path)\n",
        "plt.xlabel('Batch (1000s)')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhDx8a__2V87",
        "outputId": "8d7f94cf-4447-4213-ac8b-13ef70334ab7"
      },
      "outputs": [],
      "source": [
        "# Test the pre-trained model\n",
        "acc,recall_vals = test_model(model_resnet,test_loader_resnet,device)\n",
        "print('Test set accuracy is {:.3f}'.format(acc))\n",
        "for i in range(10):\n",
        "    print('For class {}, recall is {}'.format(classes[i],recall_vals[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hn6rYoc5T7C"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if working in Colab and you want to push new/changed files to GitHub\n",
        "# Note: you will need to manually save your current notebook to GitHub after running this cell\n",
        "# !git config --global user.email \"{git_email}\"\n",
        "# !git config --global user.name \"{'git_user'}\"\n",
        "# !git add --all #List files here that you have added or changed, other than this notebook\n",
        "# !git commit -m \"updated from colab\"\n",
        "# !git push"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_basics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
