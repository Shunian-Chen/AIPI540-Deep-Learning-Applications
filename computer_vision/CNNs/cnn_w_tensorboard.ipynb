{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OkAhQG2V8n"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3bKXytY2V8r"
      },
      "source": [
        "# CNN Example with TensorBoard\n",
        "In this demo notebook we are going to demonstrate the use of [TensorBoard](https://www.tensorflow.org/tensorboard/) with PyTorch to track and visualize metrics during model training, which is useful to evaluate model performance and compare models across experimentations.  Although TensorBoard was originally developed for use with TensorFlow, it is also supported within PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZRokJihAd3v"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if working in Colab\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"AIPI540-Deep-Learning-Applications\" # Enter repo name\n",
        "git_path = 'https://github.com/AIPI540/AIPI540-Deep-Learning-Applications.git'\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = 'computer_vision/CNNs'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77g7pHdf2V8s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the data\n",
        "if not os.path.exists('data/hymenoptera_data'):\n",
        "    url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n",
        "    urllib.request.urlretrieve(url,filename='data/hymenoptera_data.zip')\n",
        "    zip_ref = zipfile.ZipFile('data/hymenoptera_data.zip', 'r')\n",
        "    zip_ref.extractall('data/')\n",
        "    zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = 'data/hymenoptera_data'\n",
        "\n",
        "# Set up transformations for training and validation (test) data\n",
        "# For training data we will do randomized cropping to get to 224 * 224, randomized horizontal flipping, and normalization\n",
        "# For test set we will do only center cropping to get to 224 * 224 and normalization\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create Datasets for training and validation sets\n",
        "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'),\n",
        "                                          data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n",
        "                                          data_transforms['val'])\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "batch_size = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
        "                                             shuffle=False, num_workers=4)\n",
        "\n",
        "# Set up dict for dataloaders\n",
        "dataloaders = {'train':train_loader,'val':val_loader}\n",
        "# Store size of training and validation sets\n",
        "dataset_sizes = {'train':len(train_dataset),'val':len(val_dataset)}\n",
        "# Get class names associated with labels\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images, labels = iter(train_loader).next()\n",
        "images = images.numpy()\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    image = images[idx]\n",
        "    image = image.transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(\"{}\".format(class_names[labels[idx]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3EGj5MS2V8w"
      },
      "source": [
        "### Define our neural network architecture\n",
        "\n",
        "To define a neural network in PyTorch, you define the layers of a model in the function `__init__` and define the feedforward behavior of the network in the function `forward`, which takes in an input image tensor, `x`.  We will create a CNN with two convolutional layers, each followed by a ReLu activation function and then a pooling layer using MaxPool.  For each of our convolutional layers we will use a 3x3 kernel with stride=1 and padding=1 in order to maintain the input shape as it passes through the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3yfba9A2V8x"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        # Convolutional 1 layer: 3x3 kernel, stride=1, padding=1, 10 output channels / feature maps\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1, padding=1)\n",
        "        # Conv1 layer output size = (W-F+2P)/S+1 = (224-3+2)/1+1 = 224\n",
        "        # Conv1 layer output shape for one image: (10,222,222)\n",
        "        \n",
        "        # maxpool layer\n",
        "        # pool with kernel_size=2, stride=2\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Pool output shape for one image: (10,112,112)\n",
        "        \n",
        "        # Convolutional 2 layer: 3x3 kernel, stride=1, padding=1, 20 output channels / feature maps\n",
        "        self.conv2 = nn.Conv2d(in_channels=10,out_channels=10,kernel_size=3, stride=1, padding=1)\n",
        "        # Conv2 layer output size = (W-F+2P)/S+1 = (112-3+2)/1+1 = 112\n",
        "        # Conv2 layer output shape for one image: (10,112,112)\n",
        "        \n",
        "        # maxpool layer\n",
        "        # pool with kernel_size=2, stride=2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Pool output shape for one image: (10,56,56)\n",
        "        \n",
        "        # Input size: 10 * 56 * 56 = 31360 from pool2 pooling layer\n",
        "        # 2 output channels (for the 2 classes)\n",
        "        self.fc1 = nn.Linear(10*56*56, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Two convolutional layers followed by relu and then pooling\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "\n",
        "        # Flatten into a vector to feed into linear layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Linear layer\n",
        "        x = self.fc1(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUQscTuH2V8x"
      },
      "source": [
        "### Define a cost / loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q1hclmU2V8y"
      },
      "outputs": [],
      "source": [
        "net = ConvNet()\n",
        "\n",
        "# Cross entropy loss combines softmax and nn.NLLLoss() in one single class.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(net.parameters(),  lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ARCSO3Z2V8z"
      },
      "source": [
        "### Train the model\n",
        "To train our model, we perform the following four steps in a loop, using one input mini-batch at a time:  \n",
        "    1) Make a forward pass through the network to calculate the network output  \n",
        "    2) Use the network output to calculate the cost/loss  \n",
        "    3) Calculate the gradient of the cost/loss with respect to the weights by performing a backward pass through the network with loss.backward()  \n",
        "    4) Update the weights by taking a step with the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVHyrYa82V8z"
      },
      "source": [
        "If GPU is available, we can move the model (all its parameter Tensors) onto GPU. If we decided to use GPU, we also need to put all the data Tensors used in training onto GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, device, num_epochs=25):\n",
        "\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "\n",
        "    writer = SummaryWriter() # Instantiate TensorBoard\n",
        "\n",
        "    iter_num = {'train':0,'val':0} # Track total number of iterations\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the weight gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass to get outputs and calculate loss\n",
        "                # Track gradient only for training data\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backpropagation to get the gradients with respect to each weight\n",
        "                    # Only if in train\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        # Update the weights\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Convert loss into a scalar and add it to running_loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # Track number of correct predictions\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                # Iterate count of iterations\n",
        "                iter_num[phase] += 1\n",
        "\n",
        "                # Write loss for batch to TensorBoard\n",
        "                writer.add_scalar(\"{} / batch loss\".format(phase), loss.item(), iter_num[phase])\n",
        "\n",
        "            # Calculate and display average loss and accuracy for the epoch\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # Write loss and accuracy for epoch to TensorBoard\n",
        "            writer.add_scalar(\"{} / epoch loss\".format(phase), epoch_loss, epoch)\n",
        "            writer.add_scalar(\"{} / epoch accuracy\".format(phase), epoch_acc, epoch)\n",
        "\n",
        "    writer.close()\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_model(net, criterion, optimizer, dataloaders, device, num_epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch TensorBoard to visualize model performance during training cycle\n",
        "%tensorboard  --logdir=runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also add additional models to the TensorBoard to compare their performance over the training cycle.  For example, below we will use a different learning rate and then we can compare the results on the TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a different optimizer to compare\n",
        "net = ConvNet()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_model(net, criterion, optimizer, dataloaders, device, num_epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch TensorBoard to visualize model performance during training cycle\n",
        "%tensorboard  --logdir=runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "9bGA5Jgx2V83",
        "outputId": "5eef2a8b-dcf8-4c48-ce5b-6eb197072bde"
      },
      "outputs": [],
      "source": [
        "# Display a batch of predictions\n",
        "\n",
        "def visualize_results(model,dataloader,device):\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Get a batch of validation images\n",
        "        images, labels = iter(val_loader).next()\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Get predictions\n",
        "        _,preds = torch.max(model(images), 1)\n",
        "        preds = np.squeeze(preds.cpu().numpy())\n",
        "        images = images.cpu().numpy()\n",
        "\n",
        "    # Plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    for idx in np.arange(len(preds)):\n",
        "        ax = fig.add_subplot(2, len(preds)//2, idx+1, xticks=[], yticks=[])\n",
        "        image = images[idx]\n",
        "        image = image.transpose((1, 2, 0))\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "        ax.imshow(image)\n",
        "        ax.set_title(\"{} ({})\".format(class_names[preds[idx]], class_names[labels[idx]]),\n",
        "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
        "    return\n",
        "\n",
        "visualize_results(net,val_loader,device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_basics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
