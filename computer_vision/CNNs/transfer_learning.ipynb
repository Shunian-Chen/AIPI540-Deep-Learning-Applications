{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OkAhQG2V8n"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd6d5Y8L2V85"
      },
      "source": [
        "# Transfer Learning in Computer Vision\n",
        "Rather than creating our own model from scratch, we can use models which have been pre-trained on other generic tasks to take advantage of the training that has already occurred.  When we do this, we usually keep the model parameters fixed except for the last layer.  We use a new last layer (a fully connected Linear layer) which we then train on our specific task.  One way to think about this approach is that the pre-trained layers act as \"feature detectors\" which identify feature patterns within the images.  These features are then fed into the final fully-connected layer which attempts to classify the image based on the input features it receives from the rest of the model.\n",
        "\n",
        "Most commonly we use models which have been pre-treained on the ImageNet dataset.  All models pre-trained on ImageNet expect input images to be normalized in the same way, i.e. mini-batches of 3-channel RGB images with each image having shape (3 x H x W), where H and W are expected to be at least 224 pixels. The pixel values should be first scaled to a range of [0, 1] (accomplished by using `transforms.ToTensor()`) and then normalized using the mean and standard deviation of image pixel values across the image dataset.  Since it can be difficult to calculate these statistics, we often just normalize by the mean and standard deviation of the ImageNet images, which are mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. You can use the following transform to normalize: `normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])`\n",
        "\n",
        "**Notes:**\n",
        "- This notebook should be run on GPU, although the first part can run on CPU in 10-15 minutes\n",
        "\n",
        "**References:**\n",
        "- This notebook includes example code adapted from the [PyTorch docs](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
        "- This example uses a ResNet18 architecture pre-trained on ImageNet.  Read the original [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZRokJihAd3v"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if working in Colab\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"AIPI540-Deep-Learning-Applications\" # Enter repo name\n",
        "git_path = 'https://github.com/AIPI540/AIPI540-Deep-Learning-Applications.git'\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = 'computer_vision/CNNs'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77g7pHdf2V8s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transfer Learning Example\n",
        "In this example we will create a classifier capable distinguishing between images of ants and bees.  We will use a ResNet18 model which has been pre-trained on ImageNet and we will conduct fine-tuning training for our specific task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the data\n",
        "if not os.path.exists('data/hymenoptera_data'):\n",
        "    url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n",
        "    urllib.request.urlretrieve(url,filename='data/hymenoptera_data.zip')\n",
        "    zip_ref = zipfile.ZipFile('data/hymenoptera_data.zip', 'r')\n",
        "    zip_ref.extractall('data/')\n",
        "    zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set up dataloaders for our data\n",
        "We will first create and apply a pipeline of transformations to apply to our images before feeding them into the model.  We apply data augmentation techniques such as random cropping and random horizontal flipping to our training data to make our model more robust to different layouts of objects in images.  We apply normalization to both the training and validation (test) sets.  Since we are using a model pre-trained on ImageNet, our model expects input images to be normalized in the same way, i.e. mini-batches of 3-channel RGB images each having shape (3 x H x W), where H and W are expected to be at least 224 pixels. The pixel values should be first scaled to a range of [0, 1] (accomplished by using `transforms.ToTensor()`) and then normalized using the mean and standard deviation of image pixel values across the image dataset.  Since it can be difficult to calculate these statistics, we will just normalize by the mean and standard deviation of the ImageNet images, which are mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = 'data/hymenoptera_data'\n",
        "\n",
        "# Set up transformations for training and validation (test) data\n",
        "# For training data we will do randomized cropping to get to 224 * 224, randomized horizontal flipping, and normalization\n",
        "# For test set we will do only center cropping to get to 224 * 224 and normalization\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create Datasets for training and validation sets\n",
        "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'),\n",
        "                                          data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n",
        "                                          data_transforms['val'])\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "batch_size = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
        "                                             shuffle=False, num_workers=4)\n",
        "\n",
        "# Set up dict for dataloaders\n",
        "dataloaders = {'train':train_loader,'val':val_loader}\n",
        "\n",
        "# Store size of training and validation sets\n",
        "dataset_sizes = {'train':len(train_dataset),'val':len(val_dataset)}\n",
        "# Get class names associated with labels\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize a batch of images\n",
        "images, labels = iter(train_loader).next()\n",
        "print(images.shape)\n",
        "images = images.numpy()\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    image = images[idx]\n",
        "    image = image.transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(\"{}\".format(class_names[labels[idx]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we see above, the inputs in a PyTorch DataLoader are of shape [N,C,H,W) where:  \n",
        "- N = batch size  \n",
        "- C = number of channels (1 for grayscale, 3 for RGB color)  \n",
        "- H = image height  \n",
        "- W = image width"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define our model architecture\n",
        "We will used a pre-trained ResNet18 model, so our architecture has already been defined.  However, we will replace the ResNet's final layer with a new fully connected Linear layer which we will train on our specific task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate pre-trained resnet\n",
        "net = torchvision.models.resnet18(pretrained=True)\n",
        "# Shut off autograd for all layers to freeze model so the layer weights are not trained\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Get the number of inputs to final Linear layer\n",
        "num_ftrs = net.fc.in_features\n",
        "# Replace final Linear layer with a new Linear with the same number of inputs but just 2 outputs,\n",
        "# since we have 2 classes - bees and ants\n",
        "net.fc = nn.Linear(num_ftrs, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define cost function and optimizer\n",
        "We will use Cross Entropy as the cost/loss function and SGD for the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross entropy loss combines softmax and nn.NLLLoss() in one single class.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, scheduler, device, num_epochs=25):\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the weight gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass to get outputs and calculate loss\n",
        "                # Track gradient only for training data\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backpropagation to get the gradients with respect to each weight\n",
        "                    # Only if in train\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        # Update the weights\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Convert loss into a scalar and add it to running_loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # Track number of correct predictions\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Step along learning rate scheduler when in train\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate and display average loss and accuracy for the epoch\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # If model performs better on val set, save weights as the best model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:3f}'.format(best_acc))\n",
        "\n",
        "    # Load the weights from best model\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Learning rate scheduler - decay LR by a factor of 0.1 every 7 epochs\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train the model\n",
        "model_ft = train_model(net, criterion, optimizer, dataloaders, lr_scheduler, device, num_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display a batch of predictions\n",
        "def visualize_results(model,dataloader,device):\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Get a batch of validation images\n",
        "        images, labels = iter(val_loader).next()\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Get predictions\n",
        "        _,preds = torch.max(model(images), 1)\n",
        "        preds = np.squeeze(preds.cpu().numpy())\n",
        "        images = images.cpu().numpy()\n",
        "\n",
        "    # Plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    for idx in np.arange(len(preds)):\n",
        "        ax = fig.add_subplot(2, len(preds)//2, idx+1, xticks=[], yticks=[])\n",
        "        image = images[idx]\n",
        "        image = image.transpose((1, 2, 0))\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "        ax.imshow(image)\n",
        "        ax.set_title(\"{} ({})\".format(class_names[preds[idx]], class_names[labels[idx]]),\n",
        "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
        "    return\n",
        "\n",
        "visualize_results(net,val_loader,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transfer Learning on FashionMNIST\n",
        "In this example we will load a ResNet18 model and use it on the FashionMNIST dataset. The FashionMNIST dataset has a couple of differences relative to the ImageNet dataset on which pretrained models are trained:  \n",
        "- It is grayscale so has only 1 channel instead of 3. \n",
        "- It is 28 * 28 rather than 224 * 224. \n",
        "- The mean and standard deviation of pixel values across the FashionMNIST dataset are quite different from the images in ImageNet  \n",
        "- We have only 10 classes as output rather than 1000 as in ImageNet\n",
        "\n",
        "Because of those differences, we need to make two adjustments to use our pretrained model:  \n",
        "1) We need to transform our input image data to be 224*224 and normalize using the images mean and standard deviation  \n",
        "2) We need to change our net architecture to accomodate 1 input channel (grayscale, rather than the usual 3 for RGB), and 10 output classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8T4KRUO2V86"
      },
      "outputs": [],
      "source": [
        "# We need to add a transformation to our input image data\n",
        "# Resnet18 expects input images of shape 224*224\n",
        "# We also should normalize our pixel values by subracting mean and dividing by standard deviation of training set values\n",
        "\n",
        "train_data = FashionMNIST(root='data', train=True, download=True)\n",
        "\n",
        "data_transform = transforms.Compose([ transforms.Resize((224, 224)),\n",
        "                                         transforms.ToTensor(), \n",
        "                                         transforms.Normalize((train_data.data.float().mean()/255), (train_data.data.float().std()/255))])\n",
        "\n",
        "train_data_resnet = FashionMNIST(root='data', train=True,\n",
        "                                   download=True, transform=data_transform)\n",
        "\n",
        "test_data_resnet = FashionMNIST(root='data', train=False,\n",
        "                                  download=True, transform=data_transform)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader_resnet = DataLoader(train_data_resnet,batch_size=batch_size, shuffle=True)\n",
        "test_loader_resnet = DataLoader(test_data_resnet,batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Specify the image classes\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model,criterion,optimizer,train_loader,n_epochs,device):\n",
        "    \n",
        "    loss_over_time = [] # to track the loss as the network trains\n",
        "    \n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    model.train() # Set the model to training mode\n",
        "    \n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        \n",
        "        for i, data in enumerate(train_loader):\n",
        "            \n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Zero the weight gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass to get outputs\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backpropagation to get the gradients with respect to each weight\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Convert loss into a scalar and add it to running_loss\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            if i % 1000 == 999:    # print every 1000 batches\n",
        "                avg_loss = running_loss/1000\n",
        "                # record and print the avg loss over the 1000 batches\n",
        "                loss_over_time.append(avg_loss)\n",
        "                print('Epoch: {}, Batch: {}, Avg. Loss: {:.4f}'.format(epoch + 1, i+1, avg_loss))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    return loss_over_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model,test_loader,device):\n",
        "    model = model.to(device)\n",
        "    # Turn autograd off\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Set up lists to store true and predicted values\n",
        "        y_true = []\n",
        "        test_preds = []\n",
        "\n",
        "        # Calculate the predictions on the test set and add to list\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # Feed inputs through model to get raw scores\n",
        "            logits = net.forward(inputs)\n",
        "            # Convert raw scores to probabilities (not necessary since we just care about discrete probs in this case)\n",
        "            probs = F.softmax(logits,dim=1)\n",
        "            # Get discrete predictions using argmax\n",
        "            preds = np.argmax(probs.numpy(),axis=1)\n",
        "            # Add predictions and actuals to lists\n",
        "            test_preds.extend(preds)\n",
        "            y_true.extend(labels)\n",
        "\n",
        "        # Calculate the accuracy\n",
        "        test_preds = np.array(test_preds)\n",
        "        y_true = np.array(y_true)\n",
        "        test_acc = np.sum(test_preds == y_true)/y_true.shape[0]\n",
        "        \n",
        "        # Recall for each class\n",
        "        recall_vals = []\n",
        "        for i in range(10):\n",
        "            class_idx = np.argwhere(y_true==i)\n",
        "            total = len(class_idx)\n",
        "            correct = np.sum(test_preds[class_idx]==i)\n",
        "            recall = correct / total\n",
        "            recall_vals.append(recall)\n",
        "    \n",
        "    return test_acc,recall_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "1b-JfYbp2V86",
        "outputId": "c48a5cc1-1124-4539-d6a2-787bbe1863a2"
      },
      "outputs": [],
      "source": [
        "# Load a resnet18 pre-trained model\n",
        "model_resnet = torchvision.models.resnet18(pretrained=True)\n",
        "# Shut off autograd for all layers to freeze model so the layer weights are not trained\n",
        "for param in model_resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# Replace the resnet input layer to take in grayscale images (1 input channel), since it was trained on color (3 input channels)\n",
        "in_channels = 1\n",
        "model_resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Replace the resnet final layer with a new fully connected Linear layer we will train on our task\n",
        "# Number of out units is number of classes (10)\n",
        "num_ftrs = model_resnet.fc.in_features\n",
        "model_resnet.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_resnet.parameters(), lr=0.001)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cost_path = train_model(model_resnet,criterion,optimizer,train_loader_resnet,n_epochs,device)\n",
        "\n",
        "# Visualize the loss as the network trained\n",
        "plt.plot(cost_path)\n",
        "plt.xlabel('Batch (1000s)')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhDx8a__2V87",
        "outputId": "8d7f94cf-4447-4213-ac8b-13ef70334ab7"
      },
      "outputs": [],
      "source": [
        "# Test the pre-trained model\n",
        "acc,recall_vals = test_model(model_resnet,test_loader_resnet,device)\n",
        "print('Test set accuracy is {:.3f}'.format(acc))\n",
        "for i in range(10):\n",
        "    print('For class {}, recall is {}'.format(classes[i],recall_vals[i]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_basics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
