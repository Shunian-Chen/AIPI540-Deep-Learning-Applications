{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize saved article\n",
    "# with open(\"rf_article.txt\") as f:\n",
    "#     f_list = f.readlines()\n",
    "# document = [i.strip() for i in f_list]\n",
    "# document = ' '.join(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize web article \n",
    "url = 'https://towardsdatascience.com/understanding-random-forest-58381e0602d2'\n",
    "url='https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "article_text = soup.text\n",
    "h1s = soup.find_all('h1')\n",
    "h1s = [i.text for i in h1s]\n",
    "\n",
    "for title in h1s:\n",
    "    article_text = article_text.replace(title,'SECTION_BREAK',1)\n",
    "    \n",
    "article_sections = article_text.split('SECTION_BREAK')[1:]\n",
    "\n",
    "sections = {}\n",
    "for i in range(len(h1s)):\n",
    "    sections[h1s[i]]=article_sections[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize using SentenceTransformer and LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example uses LexRank (https://www.aaai.org/Papers/JAIR/Vol22/JAIR-2214.pdf)\n",
    "to create an extractive summarization of a long document.\n",
    "The document is splitted into sentences using NLTK, then the sentence embeddings are computed. We\n",
    "then compute the cosine-similarity across all possible sentence pairs.\n",
    "We then use LexRank to find the most central sentences in the document, which form our summary.\n",
    "Input document: First section from the English Wikipedia Section\n",
    "Output summary:\n",
    "Located at the southern tip of the U.S. state of New York, the city is the center of the New York metropolitan area, the largest metropolitan area in the world by urban landmass.\n",
    "New York City (NYC), often called simply New York, is the most populous city in the United States.\n",
    "Anchored by Wall Street in the Financial District of Lower Manhattan, New York City has been called both the world's leading financial center and the most financially powerful city in the world, and is home to the world's two largest stock exchanges by total market capitalization, the New York Stock Exchange and NASDAQ.\n",
    "New York City has been described as the cultural, financial, and media capital of the world, significantly influencing commerce, entertainment, research, technology, education, politics, tourism, art, fashion, and sports.\n",
    "If the New York metropolitan area were a sovereign state, it would have the eighth-largest economy in the world.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#from lexrank import degree_centrality_scores\n",
    "import sys\n",
    "sys.path.append(\"../lexrank/\")\n",
    "from lexrank.lexrank import  degree_centrality_scores\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A One-Stop Shop for Principal Component Analysis\n",
      "Num sentences: 8\n",
      "It’s safe to say that I’m not “entirely satisfied with the available texts” here.As a result, I wanted to put together the “What,” “When,” “How,” and “Why” of PCA as well as links to some of the resources that can help to further explain this topic.\n",
      "Being familiar with some or all of the following will make this article and PCA as a method easier to understand: matrix operations/linear algebra (matrix multiplication, matrix transposition, matrix inverses, matrix decomposition, eigenvectors/eigenvalues) and statistics/machine learning (standardization, variance, covariance, independence, linear regression, feature selection).\n",
      "\n",
      "What is PCA?\n",
      "Num sentences: 23\n",
      "In the GDP example above, instead of considering every single variable, we might drop all variables except the three we think will best predict what the U.S.’s gross domestic product will look like.\n",
      "But — and here’s the kicker — because these new independent variables are combinations of our old ones, we’re still keeping the most valuable parts of our old variables, even when we drop one or more of these “new” variables!Principal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables!\n",
      "\n",
      "When should I use PCA?\n",
      "Num sentences: 2\n",
      "If you answered “no” to question 3, you should not use PCA.\n",
      "Do you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration?Do you want to ensure your variables are independent of one another?Are you comfortable making your independent variables less interpretable?If you answered “yes” to all three questions, then PCA is a good method to use.\n",
      "\n",
      "How does PCA work?\n",
      "Num sentences: 73\n",
      "The red line indicates the proportion of variance explained by each feature, which is calculated by taking that principal component’s eigenvalue divided by the sum of all eigenvalues.\n",
      "As a bonus, because our eigenvectors in P* are independent of one another, each column of Z* is also independent of one another!An example from setosa.io where we transform five data points using PCA.\n",
      "\n",
      "But, like, *why* does PCA work?\n",
      "Num sentences: 17\n",
      "Bigger eigenvalues correlate with more important directions.Finally, we make an assumption that more variability in a particular direction correlates with explaining the behavior of the dependent variable.\n",
      "Then one can think of an individual eigenvector as a particular “direction” in your scatterplot of data.\n",
      "\n",
      "Are there extensions to PCA?\n",
      "Num sentences: 5\n",
      "The one I’ve most frequently seen is principal component regression, where we take our untransformed Y and regress it on the subset of Z* that we didn’t drop.\n",
      "(This is where the independence of the columns of Z* comes in; by regressing Y on Z*, we know that the required independence of independent variables will necessarily be satisfied.\n",
      "\n",
      "Conclusion\n",
      "Num sentences: 10\n",
      "Let me know what you think, especially if there are suggestions for improvement.I’ve been told that a Chinese translation of this article has been made available here.\n",
      ")I also want to give a huge h/t to the setosa.io applet for its visual and intuitive display of PCA.Edit: Thanks to Michael Matthews for noticing a typo in the formula for Z* in Step 7 above.\n",
      "\n",
      "Resources You Should Check Out:\n",
      "Num sentences: 26\n",
      "This is a list of resources I used to compile this PCA article as well as other resources I’ve generally found helpful to understand PCA.\n",
      "(PCA is covered extensively in chapters 6.3, 6.7, and 10.2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def summarize(document,model):\n",
    "    #Split the document into sentences\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    print(\"Num sentences:\", len(sentences))\n",
    "\n",
    "    #Compute the sentence embeddings\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    #Compute the pair-wise cosine similarities\n",
    "    cos_scores = util.cos_sim(embeddings, embeddings).numpy()\n",
    "\n",
    "    #Compute the centrality for each sentence\n",
    "    centrality_scores = degree_centrality_scores(cos_scores, threshold=None)\n",
    "\n",
    "    #We argsort so that the first element is the sentence with the highest score\n",
    "    most_central_sentence_indices = np.argsort(-centrality_scores)\n",
    "    \n",
    "    return [sentences[idx].strip() for idx in most_central_sentence_indices[0:2]]\n",
    "\n",
    "for section in sections.keys():\n",
    "    if len(sections[section])>0:\n",
    "        print(section)\n",
    "        top_sents = summarize(sections[section],model)\n",
    "        for sent in top_sents:\n",
    "            print(sent)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize using word counts and Networkx PageRank\n",
    "Source: https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_text):\n",
    "    article = input_text.split(\". \")\n",
    "    sentences = []\n",
    "    for sentence in article:\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(input_text, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    #sentences =  read_article(file_name)\n",
    "    sentences = preprocess(input_text)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    scores_list = list(scores.values())\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranking_idx = np.argsort(scores_list)[::-1]\n",
    "    ranked_sentence = [sentences[i] for i in ranking_idx]   \n",
    "\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i]))\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize texr\n",
    "    #print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "    summary = \". \".join(summarize_text)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A One-Stop Shop for Principal Component Analysis\n",
      "8\n",
      "It’s safe to say that I’m not “entirely satisfied with the available texts” here.As a result, I wanted to put together the “What,” “When,” “How,” and “Why” of PCA as well as links to some of the resources that can help to further explain this topic. You are writing a book because you are not entirely satisfied with the available texts.”I apply the authors’ logic here\n",
      "\n",
      "What is PCA?\n",
      "23\n",
      "Say we have ten independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.You might say, “Where does the dimensionality reduction come into play?” Well, we keep as many of the new independent variables as we want, but we drop the “least important ones.” Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important\n",
      "\n",
      "When should I use PCA?\n",
      "2\n",
      "If you answered “no” to question 3, you should not use PCA.\n",
      "\n",
      "How does PCA work?\n",
      "51\n",
      "Calculate the proportion of variance explained for each feature, sort features by proportion of variance explained and plot the cumulative proportion of variance explained as you keep more features. In the genetic data case above, I would include the first 10 principal components and drop the final three variables from Z*.Method 3: Here, we want to “find the elbow.” In the scree plot above, we see there’s a big drop in proportion of variability explained between principal component 2 and principal component 3\n",
      "\n",
      "But, like, *why* does PCA work?\n",
      "13\n",
      "Understanding how one variable is associated with another is quite powerful.Second, eigenvalues and eigenvectors are important. Eigenvectors represent directions\n",
      "\n",
      "Are there extensions to PCA?\n",
      "4\n",
      "The one I’ve most frequently seen is principal component regression, where we take our untransformed Y and regress it on the subset of Z* that we didn’t drop. However, we will need to still check our other assumptions.)The other commonly-seen variant I’ve seen is kernel PCA.\n",
      "\n",
      "Conclusion\n",
      "8\n",
      "research scooped to have a Medium presence.)I also want to give a huge h/t to the setosa.io applet for its visual and intuitive display of PCA.Edit: Thanks to Michael Matthews for noticing a typo in the formula for Z* in Step 7 above. (Thanks, Jakukyo Friel!)I want to offer many thanks to my friends Ritika Bhasker, Joseph Nelson, and Corey Smith for their suggestions and edits\n",
      "\n",
      "Resources You Should Check Out:\n",
      "14\n",
      "(This link includes Python and R.)Implementing PCA in Python with a few cool plots.Comparison of methods for implementing PCA in R.Academic Textbooks and ArticlesAn Introduction to Statistical Learning, 6th printing, by James, Witten, Hastie, and Tibshirani. This is a list of resources I used to compile this PCA article as well as other resources I’ve generally found helpful to understand PCA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for section in sections.keys():\n",
    "    print(section)\n",
    "    num_sents = len(sections[section].split('. '))\n",
    "    print(num_sents)\n",
    "    if num_sents>0:\n",
    "        if num_sents>3:\n",
    "            top_n=2\n",
    "        else:\n",
    "            top_n = num_sents-1\n",
    "        top_sents = generate_summary(sections[section],top_n=top_n)\n",
    "        print(top_sents)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize using TFIDF and Networx PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_text,stopwords):\n",
    "#     article = input_text.split(\". \")\n",
    "#     sentences = []\n",
    "#     for sentence in article:\n",
    "#         sentence = sentence.replace(\"[^a-zA-Z]\", \" \")\n",
    "#         sentences.append(sentence)\n",
    "    sentences = nltk.sent_tokenize(input_text)\n",
    "    return sentences\n",
    "\n",
    "def featurize(input_text,stopwords):\n",
    "    text_minus_stopwords = []\n",
    "    for sentence in input_text:\n",
    "        sentence_reduced = [word for word in sentence.split(' ') if word not in stopwords]\n",
    "        sentence_reduced = ' '.join(sentence_reduced)\n",
    "        text_minus_stopwords.append(sentence_reduced)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    featurized_text = vectorizer.fit_transform(text_minus_stopwords)\n",
    "    return featurized_text\n",
    "\n",
    "def sentence_similarity(featurized_text,idx1, idx2):\n",
    "    vector1 = featurized_text[idx1,:].todense().tolist()[0]\n",
    "    vector2 = featurized_text[idx2,:].todense().tolist()[0]\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(featurized_text):\n",
    "    # Create an empty similarity matrix\n",
    "    dim = featurized_text.shape[0]\n",
    "    similarity_matrix = np.zeros((dim,dim))\n",
    " \n",
    "    for idx1 in range(dim):\n",
    "        for idx2 in range(dim):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(featurized_text, idx1, idx2)\n",
    "            \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(input_text, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Preprocess text and featurize using TFIDF\n",
    "    sentences = preprocess(input_text,stop_words)\n",
    "    featurized_text = featurize(sentences,stop_words)\n",
    "\n",
    "    # Generate similary martix\n",
    "    sentence_similarity_martix = build_similarity_matrix(featurized_text)\n",
    "\n",
    "    # Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    scores_list = list(scores.values())\n",
    "\n",
    "    # Sort the scores and pick top sentences\n",
    "    ranking_idx = np.argsort(scores_list)[::-1]\n",
    "    ranked_sentences = [sentences[i] for i in ranking_idx[:top_n]]   \n",
    "    summary = \" \".join(ranked_sentences)\n",
    "    \n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A One-Stop Shop for Principal Component Analysis\n",
      "8\n",
      "It’s safe to say that I’m not “entirely satisfied with the available texts” here.As a result, I wanted to put together the “What,” “When,” “How,” and “Why” of PCA as well as links to some of the resources that can help to further explain this topic. You are writing a book because you are not entirely satisfied with the available texts.”I apply the authors’ logic here.\n",
      "\n",
      "What is PCA?\n",
      "23\n",
      "In feature extraction, we create ten “new” independent variables, where each “new” independent variable is a combination of each of the ten “old” independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.You might say, “Where does the dimensionality reduction come into play?” Well, we keep as many of the new independent variables as we want, but we drop the “least important ones.” Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important.\n",
      "\n",
      "When should I use PCA?\n",
      "2\n",
      "If you answered “no” to question 3, you should not use PCA.\n",
      "\n",
      "How does PCA work?\n",
      "51\n",
      "In the genetic data case above, I would include the first 10 principal components and drop the final three variables from Z*.Method 3: Here, we want to “find the elbow.” In the scree plot above, we see there’s a big drop in proportion of variability explained between principal component 2 and principal component 3. Calculate the proportion of variance explained for each feature, sort features by proportion of variance explained and plot the cumulative proportion of variance explained as you keep more features.\n",
      "\n",
      "But, like, *why* does PCA work?\n",
      "13\n",
      "Eigenvectors represent directions. Understanding how one variable is associated with another is quite powerful.Second, eigenvalues and eigenvectors are important.\n",
      "\n",
      "Are there extensions to PCA?\n",
      "4\n",
      ")The other commonly-seen variant I’ve seen is kernel PCA. The one I’ve most frequently seen is principal component regression, where we take our untransformed Y and regress it on the subset of Z* that we didn’t drop.\n",
      "\n",
      "Conclusion\n",
      "8\n",
      ")I want to offer many thanks to my friends Ritika Bhasker, Joseph Nelson, and Corey Smith for their suggestions and edits. )I also want to give a huge h/t to the setosa.io applet for its visual and intuitive display of PCA.Edit: Thanks to Michael Matthews for noticing a typo in the formula for Z* in Step 7 above.\n",
      "\n",
      "Resources You Should Check Out:\n",
      "14\n",
      "(PCA is covered in chapter 7.5. This is a list of resources I used to compile this PCA article as well as other resources I’ve generally found helpful to understand PCA.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for section in sections.keys():\n",
    "    print(section)\n",
    "    num_sents = len(sections[section].split('. '))\n",
    "    print(num_sents)\n",
    "    if num_sents>0:\n",
    "        if num_sents>3:\n",
    "            top_n=2\n",
    "        else:\n",
    "            top_n = num_sents-1\n",
    "        top_sents = generate_summary(sections[section],top_n=top_n)\n",
    "        print(top_sents)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def summarize_bart(input_text):\n",
    "#     if len(input_text)>1024:\n",
    "#         input_text = input_text[:1024]\n",
    "#     summarizer = pipeline(\"summarization\")\n",
    "#     summarized = summarizer(input_text, min_length=50, max_length=400)\n",
    "#     return summarized[0]['summary_text']\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"], max_length=300, min_length=50, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A One-Stop Shop for Principal Component Analysis\n",
      "8\n",
      "</s><s> Principal component analysis (PCA) is an important technique to understand in the fields of statistics and data science. The algorithm we’ll cover is pretty technical. Being familiar with some or all of the following will make this article and PCA as a method easier to understand.</s>\n",
      "\n",
      "What is PCA?\n",
      "23\n",
      "</s><s> You have a lot of variables to consider when trying to predict what the U.S. GDP will look like in 2017. You might ask, “How do I take all of the variables I’ve collected and focus on only a few of them? In technical terms, you want to “reduce the dimension of your feature space”</s>\n",
      "\n",
      "When should I use PCA?\n",
      "2\n",
      "</s><s> Do you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration? Are you comfortable making your independent variables less interpretable? If you answered “yes” to all three questions, then PCA is a good method to use.</s>\n",
      "\n",
      "How does PCA work?\n",
      "51\n",
      "</s><s> The “red direction” of our data and its “magnitude” (or how “important” each direction is) is the more important one. By identifying which “directions” are most important, we can compress or project our data into a smaller space by dropping the “least important” directions.</s>\n",
      "\n",
      "But, like, *why* does PCA work?\n",
      "13\n",
      "</s><s> PCA is a very technical method relying on in-depth linear algebra algorithms. The more variability in a particular direction is, theoretically, indicative of something important we want to detect. PCA combines our predictors and allows us to drop the eigenvectors that are relatively unimportant.</s>\n",
      "\n",
      "Are there extensions to PCA?\n",
      "4\n",
      "</s><s> The one I’ve most frequently seen is principal component regression, where we take our untransformed Y and regress it on the subset of Z* that we didn’t drop. The other commonly-seen variant is kernel PCA.</s>\n",
      "\n",
      "Conclusion\n",
      "8\n",
      "</s><s> Thanks to Ritika Bhasker, Joseph Nelson, Corey Smith for their suggestions and edits. I also want to give a huge h/t to the setosa.io applet for its visual and intuitive display of PCA.</s>\n",
      "\n",
      "Resources You Should Check Out:\n",
      "14\n",
      "</s><s> This is a list of resources I used to compile this PCA article as well as other resources I've generally found helpful to understand PCA. If you know of any resources that would be a good inclusion to this list, please leave a comment and I’ll add them.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for section in sections.keys():\n",
    "    print(section)\n",
    "    num_sents = len(sections[section].split('. '))\n",
    "    print(num_sents)\n",
    "    top_sents = summarize_bart(sections[section])\n",
    "    print(top_sents)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
