{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize saved article\n",
    "# with open(\"rf_article.txt\") as f:\n",
    "#     f_list = f.readlines()\n",
    "# document = [i.strip() for i in f_list]\n",
    "# document = ' '.join(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize web article \n",
    "url = 'https://towardsdatascience.com/understanding-random-forest-58381e0602d2'\n",
    "url='https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "article_text = soup.text\n",
    "h1s = soup.find_all('h1')\n",
    "h1s = [i.text for i in h1s]\n",
    "\n",
    "for title in h1s:\n",
    "    article_text = article_text.replace(title,'SECTION_BREAK',1)\n",
    "    \n",
    "article_sections = article_text.split('SECTION_BREAK')[1:]\n",
    "\n",
    "sections = {}\n",
    "for i in range(len(h1s)):\n",
    "    sections[h1s[i]]=article_sections[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize using word counts and Networkx PageRank\n",
    "Source: https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_text):\n",
    "    article = input_text.split(\". \")\n",
    "    sentences = []\n",
    "    for sentence in article:\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "# Compute similarity of a pair of sentences\n",
    "\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # Build vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # Build vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(input_text, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Split text\n",
    "    #sentences =  read_article(file_name)\n",
    "    sentences = preprocess(input_text)\n",
    "\n",
    "    # Step 2 - Generate similarity matrix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity matrix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    scores_list = list(scores.values())\n",
    "\n",
    "    # Step 4 - Sort and pick top sentences\n",
    "    ranking_idx = np.argsort(scores_list)[::-1]\n",
    "    ranked_sentence = [sentences[i] for i in ranking_idx]   \n",
    "\n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(\" \".join(ranked_sentence[i]))\n",
    "\n",
    "    summary = \". \".join(summarize_text)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A One-Stop Shop for Principal Component Analysis\n",
      "8\n",
      "It’s safe to say that I’m not “entirely satisfied with the available texts” here.As a result, I wanted to put together the “What,” “When,” “How,” and “Why” of PCA as well as links to some of the resources that can help to further explain this topic. You are writing a book because you are not entirely satisfied with the available texts.”I apply the authors’ logic here\n",
      "\n",
      "What is PCA?\n",
      "23\n",
      "Say we have ten independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.You might say, “Where does the dimensionality reduction come into play?” Well, we keep as many of the new independent variables as we want, but we drop the “least important ones.” Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important\n",
      "\n",
      "When should I use PCA?\n",
      "2\n",
      "If you answered “no” to question 3, you should not use PCA.\n",
      "\n",
      "How does PCA work?\n",
      "51\n",
      "Calculate the proportion of variance explained for each feature, sort features by proportion of variance explained and plot the cumulative proportion of variance explained as you keep more features. In the genetic data case above, I would include the first 10 principal components and drop the final three variables from Z*.Method 3: Here, we want to “find the elbow.” In the scree plot above, we see there’s a big drop in proportion of variability explained between principal component 2 and principal component 3\n",
      "\n",
      "But, like, *why* does PCA work?\n",
      "13\n",
      "Understanding how one variable is associated with another is quite powerful.Second, eigenvalues and eigenvectors are important. Eigenvectors represent directions\n",
      "\n",
      "Are there extensions to PCA?\n",
      "4\n",
      "The one I’ve most frequently seen is principal component regression, where we take our untransformed Y and regress it on the subset of Z* that we didn’t drop. However, we will need to still check our other assumptions.)The other commonly-seen variant I’ve seen is kernel PCA.\n",
      "\n",
      "Conclusion\n",
      "8\n",
      "research scooped to have a Medium presence.)I also want to give a huge h/t to the setosa.io applet for its visual and intuitive display of PCA.Edit: Thanks to Michael Matthews for noticing a typo in the formula for Z* in Step 7 above. (Thanks, Jakukyo Friel!)I want to offer many thanks to my friends Ritika Bhasker, Joseph Nelson, and Corey Smith for their suggestions and edits\n",
      "\n",
      "Resources You Should Check Out:\n",
      "14\n",
      "(This link includes Python and R.)Implementing PCA in Python with a few cool plots.Comparison of methods for implementing PCA in R.Academic Textbooks and ArticlesAn Introduction to Statistical Learning, 6th printing, by James, Witten, Hastie, and Tibshirani. This is a list of resources I used to compile this PCA article as well as other resources I’ve generally found helpful to understand PCA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for section in sections.keys():\n",
    "    print(section)\n",
    "    num_sents = len(sections[section].split('. '))\n",
    "    print(num_sents)\n",
    "    if num_sents>0:\n",
    "        if num_sents>3:\n",
    "            top_n=2\n",
    "        else:\n",
    "            top_n = num_sents-1\n",
    "        top_sents = generate_summary(sections[section],top_n=top_n)\n",
    "        print(top_sents)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize using TFIDF and Networx PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_text,stopwords):\n",
    "#     article = input_text.split(\". \")\n",
    "#     sentences = []\n",
    "#     for sentence in article:\n",
    "#         sentence = sentence.replace(\"[^a-zA-Z]\", \" \")\n",
    "#         sentences.append(sentence)\n",
    "    sentences = nltk.sent_tokenize(input_text)\n",
    "    return sentences\n",
    "\n",
    "def featurize(input_text,stopwords):\n",
    "    text_minus_stopwords = []\n",
    "    for sentence in input_text:\n",
    "        sentence_reduced = [word for word in sentence.split(' ') if word not in stopwords]\n",
    "        sentence_reduced = ' '.join(sentence_reduced)\n",
    "        text_minus_stopwords.append(sentence_reduced)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    featurized_text = vectorizer.fit_transform(text_minus_stopwords)\n",
    "    return featurized_text\n",
    "\n",
    "def sentence_similarity(featurized_text,idx1, idx2):\n",
    "    vector1 = featurized_text[idx1,:].todense().tolist()[0]\n",
    "    vector2 = featurized_text[idx2,:].todense().tolist()[0]\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(featurized_text):\n",
    "    # Create an empty similarity matrix\n",
    "    dim = featurized_text.shape[0]\n",
    "    similarity_matrix = np.zeros((dim,dim))\n",
    " \n",
    "    for idx1 in range(dim):\n",
    "        for idx2 in range(dim):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(featurized_text, idx1, idx2)\n",
    "            \n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(input_text, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Preprocess text and featurize using TFIDF\n",
    "    sentences = preprocess(input_text,stop_words)\n",
    "    featurized_text = featurize(sentences,stop_words)\n",
    "\n",
    "    # Generate similary martix\n",
    "    sentence_similarity_martix = build_similarity_matrix(featurized_text)\n",
    "\n",
    "    # Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    scores_list = list(scores.values())\n",
    "\n",
    "    # Sort the scores and pick top sentences\n",
    "    ranking_idx = np.argsort(scores_list)[::-1]\n",
    "    ranked_sentences = [sentences[i] for i in ranking_idx[:top_n]]   \n",
    "    summary = \" \".join(ranked_sentences)\n",
    "    \n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A One-Stop Shop for Principal Component Analysis\n",
      "8\n",
      "It’s safe to say that I’m not “entirely satisfied with the available texts” here.As a result, I wanted to put together the “What,” “When,” “How,” and “Why” of PCA as well as links to some of the resources that can help to further explain this topic. You are writing a book because you are not entirely satisfied with the available texts.”I apply the authors’ logic here.\n",
      "\n",
      "What is PCA?\n",
      "23\n",
      "In feature extraction, we create ten “new” independent variables, where each “new” independent variable is a combination of each of the ten “old” independent variables. However, we create these new independent variables in a specific way and order these new variables by how well they predict our dependent variable.You might say, “Where does the dimensionality reduction come into play?” Well, we keep as many of the new independent variables as we want, but we drop the “least important ones.” Because we ordered the new variables by how well they predict our dependent variable, we know which variable is the most important and least important.\n",
      "\n",
      "When should I use PCA?\n",
      "2\n",
      "If you answered “no” to question 3, you should not use PCA.\n",
      "\n",
      "How does PCA work?\n",
      "51\n",
      "In the genetic data case above, I would include the first 10 principal components and drop the final three variables from Z*.Method 3: Here, we want to “find the elbow.” In the scree plot above, we see there’s a big drop in proportion of variability explained between principal component 2 and principal component 3. Calculate the proportion of variance explained for each feature, sort features by proportion of variance explained and plot the cumulative proportion of variance explained as you keep more features.\n",
      "\n",
      "But, like, *why* does PCA work?\n",
      "13\n",
      "Eigenvectors represent directions. Understanding how one variable is associated with another is quite powerful.Second, eigenvalues and eigenvectors are important.\n",
      "\n",
      "Are there extensions to PCA?\n",
      "4\n",
      ")The other commonly-seen variant I’ve seen is kernel PCA. The one I’ve most frequently seen is principal component regression, where we take our untransformed Y and regress it on the subset of Z* that we didn’t drop.\n",
      "\n",
      "Conclusion\n",
      "8\n",
      ")I want to offer many thanks to my friends Ritika Bhasker, Joseph Nelson, and Corey Smith for their suggestions and edits. )I also want to give a huge h/t to the setosa.io applet for its visual and intuitive display of PCA.Edit: Thanks to Michael Matthews for noticing a typo in the formula for Z* in Step 7 above.\n",
      "\n",
      "Resources You Should Check Out:\n",
      "14\n",
      "(PCA is covered in chapter 7.5. This is a list of resources I used to compile this PCA article as well as other resources I’ve generally found helpful to understand PCA.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for section in sections.keys():\n",
    "    print(section)\n",
    "    num_sents = len(sections[section].split('. '))\n",
    "    print(num_sents)\n",
    "    if num_sents>0:\n",
    "        if num_sents>3:\n",
    "            top_n=2\n",
    "        else:\n",
    "            top_n = num_sents-1\n",
    "        top_sents = generate_summary(sections[section],top_n=top_n)\n",
    "        print(top_sents)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "def summarize_bart(input_text):\n",
    "#     if len(input_text)>1024:\n",
    "#         input_text = input_text[:1024]\n",
    "#     summarizer = pipeline(\"summarization\")\n",
    "#     summarized = summarizer(input_text, min_length=50, max_length=400)\n",
    "#     return summarized[0]['summary_text']\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"], max_length=300, min_length=50, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A One-Stop Shop for Principal Component Analysis\n",
      "8\n",
      "</s><s> Principal component analysis (PCA) is an important technique to understand in the fields of statistics and data science. The algorithm we’ll cover is pretty technical. Being familiar with some or all of the following will make this article and PCA as a method easier to understand.</s>\n",
      "\n",
      "What is PCA?\n",
      "23\n",
      "</s><s> You have a lot of variables to consider when trying to predict what the U.S. GDP will look like in 2017. You might ask, “How do I take all of the variables I’ve collected and focus on only a few of them? In technical terms, you want to “reduce the dimension of your feature space”</s>\n",
      "\n",
      "When should I use PCA?\n",
      "2\n",
      "</s><s> Do you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration? Are you comfortable making your independent variables less interpretable? If you answered “yes” to all three questions, then PCA is a good method to use.</s>\n",
      "\n",
      "How does PCA work?\n",
      "51\n",
      "</s><s> The “red direction” of our data and its “magnitude” (or how “important” each direction is) is the more important one. By identifying which “directions” are most important, we can compress or project our data into a smaller space by dropping the “least important” directions.</s>\n",
      "\n",
      "But, like, *why* does PCA work?\n",
      "13\n",
      "</s><s> PCA is a very technical method relying on in-depth linear algebra algorithms. The more variability in a particular direction is, theoretically, indicative of something important we want to detect. PCA combines our predictors and allows us to drop the eigenvectors that are relatively unimportant.</s>\n",
      "\n",
      "Are there extensions to PCA?\n",
      "4\n",
      "</s><s> The one I’ve most frequently seen is principal component regression, where we take our untransformed Y and regress it on the subset of Z* that we didn’t drop. The other commonly-seen variant is kernel PCA.</s>\n",
      "\n",
      "Conclusion\n",
      "8\n",
      "</s><s> Thanks to Ritika Bhasker, Joseph Nelson, Corey Smith for their suggestions and edits. I also want to give a huge h/t to the setosa.io applet for its visual and intuitive display of PCA.</s>\n",
      "\n",
      "Resources You Should Check Out:\n",
      "14\n",
      "</s><s> This is a list of resources I used to compile this PCA article as well as other resources I've generally found helpful to understand PCA. If you know of any resources that would be a good inclusion to this list, please leave a comment and I’ll add them.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for section in sections.keys():\n",
    "    print(section)\n",
    "    num_sents = len(sections[section].split('. '))\n",
    "    print(num_sents)\n",
    "    top_sents = summarize_bart(sections[section])\n",
    "    print(top_sents)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
