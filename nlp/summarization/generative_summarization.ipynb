{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Text Summarization\n",
    "Abstractive text summarization methods attempt to create a summary of a document by generating shorter text which captures the main points of the source document but is much shorter in length.  Unlike extractive summarization methods, the text in summaries produced using abstractive methods may include new phrases and sentences which did not appear in the source text.\n",
    "\n",
    "The current state-of-the-art approach for abstractive text summarization uses transformer models which have been pre-trained or fine-tuned on large datasets with documents suitable for the summarization task.  In this notebook we will use the open source [Hugging Face library](https://huggingface.co) to load and use a transformer model.\n",
    "\n",
    "**Notes:**  \n",
    "- This does not need to be run on GPU, although it will take a few minutes to run on CPU\n",
    "- This notebook uses a [DistilBart model](https://arxiv.org/pdf/2010.13002.pdf), but you can also use other Bart models or Google's T5 instead  \n",
    "\n",
    "**References:**  \n",
    "- Review the Hugging Face [summarization documentation](https://huggingface.co/docs/transformers/task_summary#summarization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document to summarize\n",
    "We will use BeautifulSoup to get the content of an article on the web and strip the text content from the hmtl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article\n",
    "url = 'https://en.wikipedia.org/wiki/Random_forest'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Extract body text from article\n",
    "bodytext = soup.find_all('p')\n",
    "bodytext = [i.text for i in bodytext]\n",
    "article_text = ' '.join(bodytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model & associated tokenizer\n",
    "We will use the open source Hugging Face library to load a pre-trained transformer model from their Model Zoo.  Hugging Face recommends using a Bart or Google's T5 model for summarization tasks.  Below we will use a [DistilBart model](https://arxiv.org/pdf/2010.13002.pdf), but you can try others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate summary\n",
    "Now that our model is loaded we can use it to generate summary text.  We first tokenize the article text and then feed the tokenized text into the model to generate the summary.  We are able to specify a desired minimum and maximum length for the output summary.  Note that the DistilBart model can accept a maximum input sequence length of 1024, and so we must either truncate our source document to 1024 characters or create batches of 1024 characters and summarize each batch, and then combine for the full document summary.\n",
    "\n",
    "Let's first try it by simply truncating our input text to 1024 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_summary(input_text,min_length,max_length):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=min_length, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the source document: 24552\n",
      "Length of the summary: 375\n",
      "Summary: \n",
      "\n",
      "Random forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees.\n"
     ]
    }
   ],
   "source": [
    "# Set desired min and max length for summary\n",
    "min_length = 50\n",
    "max_length = 400\n",
    "# Generate summary\n",
    "summary = truncate_summary(article_text,min_length,max_length)\n",
    "# Clean up output formatting\n",
    "summary = summary.split('</s>')[-2].split('<s>')[-1].strip()\n",
    "\n",
    "print('Length of the source document: {}'.format(len(article_text)))\n",
    "print('Length of the summary: {}'.format(len(summary)))\n",
    "print('Summary: ')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try another approach of \"chunking\" our document into chunks of 1024 characters and summarizing each one, and then combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_summary(input_text,min_chunk_len,max_chunk_len):\n",
    "    # Separate the input text into chunks\n",
    "    chunked_inputs = [input_text[i:i+1024] for i in range(0,len(input_text),1024)]\n",
    "    summary = ''\n",
    "    # Get input for each chunk\n",
    "    for chunk in chunked_inputs:\n",
    "        chunk_summary = truncate_summary(chunk,min_chunk_len,max_chunk_len)\n",
    "        chunk_summary = chunk_summary.split('</s>')[-2].split('<s>')[-1].strip()\n",
    "        summary += (' '+chunk_summary)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the source document: 24552\n",
      "Length of the summary: 6299\n",
      "Summary: \n",
      " Random forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. The first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method. Random forests are frequently used as \"blackbox\" models in businesses. They generate reasonable predictions across a wide range of data while requiring little configuration. The general method of random decision forests was first proposed by Ho in 1995. Breiman's notion of random forests was influenced by the work of Amit and Geman[13] who introduced the idea of searching over a random subset of the available decisions when splitting a node. In this method a forest of trees is grown, and variation among the trees is introduced by projecting the training data into a randomly chosen subspace before fitting each tree or each node. Decision trees are a popular method for various machine learning tasks. Tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\" Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability. Forests are like the pulling together of decision tree algorithm efforts. Given a training set X = x1,..., xn with responses Y = y1, yn, and xn, B times, selects a random sample with replacement of the training set and fits trees to these samples. After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x'. The number of samples/trees, B, is a free parameter. An optimal number of trees B can be found using cross-validation or by observing the out-of-bag error: the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.[16] Typically, for a classification problem with p features, âˆšp (rounded down) features are used in each split. Adding one further step of randomization yields extremely randomized trees, or ExtraTrees. Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way. Default values for this parameter are the number of features in the model. The first step in measuring the variable importance in a data set is to fit a random forest to the data. During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest. Errors on an independent test set can be substituted if bagging is not used during training. The statistical definition of the variable importance measure was given and analyzed by Zhu et al. This method of determining variable importance has some drawbacks. For data including categorical variables with different number of levels, random forests are biased. The neighborhood of x' depends in a complex way on the structure of the trees, a weighted neighborhood scheme. For any particular x', the weights for points ï¿½ï¿½ must sum to one\" Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature. A random forest dissimilarity measure can be attractive because it handles mixed variable types very well. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data. Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests. Random Forest Kernel can outperform state-of-art kernel methods. Centered forest is a simplified model for Breiman's original random forest. Uniform random forest performs splits at the center of the cell along the pre-chosen attribute. Uniform forest[35] is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature. Uniform forest is a parameter of the algorithm. A random regression forest is an ensemble of randomized regression trees. Random regression trees can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate. For regression trees, we have a random forest designed with randomness. The random forest is the cell containing a cell containing  a random cell. Random regression forest has two levels of averaging, first over the samples in the target cell of a tree, then over all trees. The contributions of observations that are in cells with a high density are smaller than that of observations which belong to less populated cells. The construction of the Centered KeRF of level level level Â â€˜The construction of CenteredÂ KeRF is based on the connection function of the KeRF. Uniform KeRF is built in the same way as uniform forest, except that predictions are made by KeRF and random forests. Predictions are close if the number of points in each cell is controlled. Assume that there exist sequences. that. such that, almost surely. such that. observations in each cell is bounded:. Then almost surely, almost. surely, then almost surely the observations are bounded:. Assume   that  Â there exist sequences. with finite variance  with finite variances. Decision trees are among a fairly small family of machine learning models that are easily interpretable along with linear models, rule-based models, and at least linear models. Random forests often achieve higher accuracy than a single decision tree, they sacrifice the intrinsic interpretability present in decision trees. Using random forest may not enhance the accuracy of the base learner. Some model compression techniques allow transforming a random forest into a minimal \"born-again\" decision tree that faithfully reproduces the same decision function.\n"
     ]
    }
   ],
   "source": [
    "# Set desired min and max length for summary\n",
    "min_length = 25\n",
    "max_length = 100\n",
    "# Generate summary\n",
    "summary = chunked_summary(article_text,min_length,max_length)\n",
    "\n",
    "print('Length of the source document: {}'.format(len(article_text)))\n",
    "print('Length of the summary: {}'.format(len(summary)))\n",
    "print('Summary: ')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
