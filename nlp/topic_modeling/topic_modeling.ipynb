{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "\n",
    "References:  \n",
    "- https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea\n",
    "- https://jaketae.github.io/study/zero-shot-classification/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "#from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://towardsdatascience.com/understanding-random-forest-58381e0602d2'\n",
    "#url='https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "article_text = soup.text\n",
    "\n",
    "#h1s = soup.find_all(['h1','h2','h3'])\n",
    "#h1s = [i.text for i in h1s]\n",
    "bodytext = soup.find_all('p')\n",
    "bodytext = [i.text for i in bodytext]\n",
    "article_text = ' '.join(bodytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_range = (1, 2)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "vectorizer = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words,token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}').fit([article_text])\n",
    "candidates = vectorizer.get_feature_names()\n",
    "\n",
    "# Get noun phrases from article\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(article_text)\n",
    "noun_phrases = set(chunk.text.strip().lower() for chunk in doc.noun_chunks)\n",
    "\n",
    "# Get nouns from article\n",
    "nouns = set()\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.add(token.text)\n",
    "        \n",
    "all_nouns = nouns.union(noun_phrases)\n",
    "\n",
    "# Filter candidate topics to only those in the nouns set\n",
    "candidates = [c for c in candidates if c in all_nouns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer approach with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['massive alpha',\n",
       " 'classification algorithms',\n",
       " 'machine learning',\n",
       " 'data science',\n",
       " 'data scientist']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([article_text])\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['correct predictions',\n",
       " 'decision trees',\n",
       " 'massive alpha',\n",
       " 'machine learning',\n",
       " 'data scientist']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_sum_sim(doc_embedding, word_embeddings,top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]\n",
    "\n",
    "nr_candidates=10\n",
    "keywords = max_sum_sim(doc_embedding, candidate_embeddings, top_n,nr_candidates)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random', 'forest', 'trees', 'tree', 'random forest', 'data', 'game', 'feature', 'decision', 'individual', 'split', 'different', 'node', 'make', 'expected', 'features', 'decision tree', 'training', 'like', 'uncorrelated']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random forest',\n",
       " 'trees',\n",
       " 'decision tree',\n",
       " 'data',\n",
       " 'game',\n",
       " 'feature',\n",
       " 'individual',\n",
       " 'split',\n",
       " 'different',\n",
       " 'node',\n",
       " 'make',\n",
       " 'expected',\n",
       " 'features',\n",
       " 'training',\n",
       " 'like',\n",
       " 'uncorrelated']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 1\n",
    "text_vectorized = vectorizer.transform([article_text])\n",
    "lda_model = LatentDirichletAllocation(n_components=top_n, max_iter=10, learning_method='online')\n",
    "data_lda = lda_model.fit_transform(text_vectorized)\n",
    "for topic in lda_model.components_:\n",
    "    topics = [vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1][:20]]\n",
    "\n",
    "# Combine 1-grams to 2-grams when 2-gram is in topics list\n",
    "newtopics = []\n",
    "wordsused = []\n",
    "for i,top1 in enumerate(topics):\n",
    "    for j,top2 in enumerate(topics[i+1:]):\n",
    "        if ' '.join([top1,top2]) in topics:\n",
    "            if ' '.join([top1,top2]) not in newtopics:\n",
    "                newtopics.append(' '.join([top1,top2]))\n",
    "                wordsused.extend([top1,top2])\n",
    "        elif ' '.join([top2,top1]) in topics:\n",
    "            if ' '.join([top2,top1]) not in newtopics:\n",
    "                newtopics.append(' '.join([top2,top1]))\n",
    "                wordsused.extend([top1,top2])\n",
    "    if (top1 not in wordsused) and (top1 not in newtopics):\n",
    "        newtopics.append(top1)\n",
    "newtopics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random forest',\n",
       " 'decision tree',\n",
       " 'trees',\n",
       " 'data',\n",
       " 'game',\n",
       " 'feature',\n",
       " 'individual',\n",
       " 'different',\n",
       " 'split',\n",
       " 'features',\n",
       " 'make',\n",
       " 'expected',\n",
       " 'node',\n",
       " 'like',\n",
       " 'uncorrelated',\n",
       " 'model']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 1\n",
    "text_vectorized = vectorizer.transform([article_text])\n",
    "nmf_model = NMF(n_components=top_n,max_iter=500)\n",
    "data_nmf = nmf_model.fit_transform(text_vectorized)\n",
    "for topic in nmf_model.components_:\n",
    "    topics = [vectorizer.get_feature_names()[i] for i in topic.argsort()[::-1][:20]]\n",
    "\n",
    "# Combine 1-grams to 2-grams when 2-gram is in topics list\n",
    "newtopics = []\n",
    "wordsused = []\n",
    "for i,top1 in enumerate(topics):\n",
    "    for j,top2 in enumerate(topics[i+1:]):\n",
    "        if ' '.join([top1,top2]) in topics:\n",
    "            if ' '.join([top1,top2]) not in newtopics:\n",
    "                newtopics.append(' '.join([top1,top2]))\n",
    "                wordsused.extend([top1,top2])\n",
    "        elif ' '.join([top2,top1]) in topics:\n",
    "            if ' '.join([top2,top1]) not in newtopics:\n",
    "                newtopics.append(' '.join([top2,top1]))\n",
    "                wordsused.extend([top1,top2])\n",
    "    if (top1 not in wordsused) and (top1 not in newtopics):\n",
    "        newtopics.append(top1)\n",
    "newtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
