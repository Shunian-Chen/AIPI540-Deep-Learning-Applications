{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af855268",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF)\n",
    "Topic modeling involves identifying the primary topic(s) for a text document or within a set of documents.  There are topic modeling approaches which use supervised learning to perform multi-class multi-label classification of documents, but this type of approach requires a large labeled training dataset, which we do not always have available.  In this notebook we will explore unsupervised topic modeling approaches using LDA and NMF.  The \"topics\" produced by unsupervised topic modeling techniques are actually clusters of similar words found in the document(s). The topic model discovers these word clusters based on the frequency of the words in each document and attempts to model what the topics might be and what each document's balance of topics is.\n",
    "\n",
    "Topic modeling can be used to extract the main topic(s) from a single document or a collection of documents - in this notebook we will demonstrate topic modeling for a small collection of recent news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c3cb6",
   "metadata": {},
   "source": [
    "## Get documents to tag with topics\n",
    "We will use BeautifulSoup to get the content of a few articles from the web and strip the text content from the hmtl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article\n",
    "article_urls = ['https://www.cbssports.com/college-basketball/news/duke-basketballs-game-vs-clemson-postponed-due-to-positive-covid-19-tests-in-blue-devils-program/',\n",
    "                'https://www.cnbc.com/2021/04/26/apple-announces-1-billion-north-carolina-campus.html',\n",
    "                'https://www.wral.com/coronavirus/100-of-duke-health-patients-in-icu-or-on-life-saving-treatment-are-unvaccinated-hospital-cases-rising/20045587/',\n",
    "                'https://www.macworld.com/article/561815/apple-2022-ar-headset-homepod-mac.html',\n",
    "                'https://www.dukechronicle.com/article/2021/12/duke-university-classes-online-spring-2022-semester-move-in-entry-testing-covid-vaccine-booster-mandate-require']\n",
    "article_text = []\n",
    "for url in article_urls:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Extract body text from article\n",
    "    bodytext = soup.find_all('p')\n",
    "    bodytext = [i.text for i in bodytext]\n",
    "    bodytext = ' '.join(bodytext)\n",
    "    article_text.append(bodytext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features using Word Counts or TF-IDF\n",
    "Before we can apply a topic model on our set of documents, we must first convert each document into a numeric feature vector.  We can use count vectorization or TF-IDF to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "196d4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(documents, vectorizer_type='count'):\n",
    "    # Use both 1-grams and 2-grams\n",
    "    n_gram_range = (1, 2)\n",
    "\n",
    "    if vectorizer_type == 'count':\n",
    "        vectorizer = CountVectorizer(max_df=0.6,ngram_range=n_gram_range,\n",
    "                                    stop_words=stopwords.words('english'))\n",
    "        feature_vecs = vectorizer.fit_transform(documents)\n",
    "        feature_vecs = feature_vecs.todense().tolist()\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(max_df=0.6,ngram_range=n_gram_range,\n",
    "                                    stop_words=stopwords.words('english'))\n",
    "        feature_vecs = vectorizer.fit_transform(documents)\n",
    "        feature_vecs = feature_vecs.todense().tolist()\n",
    "        \n",
    "    return feature_vecs, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model topics using LDA or NMF\n",
    "Now that our documents are represented by numeric feature vectors, we can apply our topic model.  When fitting the model, we need to select the number of topics we wish to include in our final list, which will correspond to the top n components extracted by LDA/NMF.\n",
    "\n",
    "Each topic is represented by a cluster of similar keywords.  When printing our topic we can also select how many keywords we want to include to represent each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e6060f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_topics(vectorized_text,vectorizer,n_topics,n_words,model_type='lda'):\n",
    "    if model_type=='lda':\n",
    "        # Perform LDA \n",
    "        model = LatentDirichletAllocation(n_components=n_topics,learning_method='online', random_state=1)\n",
    "        model.fit_transform(vectorized_text)\n",
    "    else:\n",
    "        model = NMF(n_components=n_topics,max_iter=500)\n",
    "        model.fit_transform(vectorized_text)\n",
    "\n",
    "    # Get the main keywords and scores corresponding to each topic\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for comp in model.components_:\n",
    "        # Get the top keywords for each topic\n",
    "        sorted_words = [vocab[score] for score in np.argsort(comp)[::-1]][:n_words]\n",
    "        # Get the scores for each top keyword\n",
    "        sorted_scores = np.sort(comp)[::-1][:n_words]\n",
    "        words_scores = zip(sorted_words,sorted_scores)\n",
    "        topics.append(words_scores)\n",
    "\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3682e9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 keywords: ['apple', 'covid 19', '19', 'health']\n",
      "Topic 1 keywords: ['apple', '000', 'engineering', 'campus']\n",
      "Topic 2 keywords: ['wild', 'proportional', 'exec', 'jan asked']\n",
      "Topic 3 keywords: ['cbs', 'sports', 'cbs sports', 'positive']\n",
      "Topic 4 keywords: ['jan', 'students', 'test', 'email']\n"
     ]
    }
   ],
   "source": [
    "feature_vecs, vectorizer = vectorize(article_text,vectorizer_type='tfidf')\n",
    "topics = model_topics(feature_vecs,vectorizer,n_topics=5,n_words=4,model_type='lda')\n",
    "for i,topic in enumerate(topics):\n",
    "    print('Topic {} keywords: {}'.format(i,[word[0] for word in topic]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7a474",
   "metadata": {},
   "source": [
    "## Post-process topics\n",
    "We may want to do some post-processing on the topics which our topic model identifies.  For example, we may have cases where a 1-gram is included twice in our keyword list for a topic - once by itself and then again as part of a 2-gram.  We may choose to remove the 1-gram if the word is included in a 2-gram since the two-gram may be more descriptive (e.g. \"random\" and \"random forest\").  We may also want to check and make sure we don't have any numeric-only keywords listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5a33d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe_topics(topics,n_words):\n",
    "    newtopics = []\n",
    "    wordsused = []\n",
    "    for topic in topics:\n",
    "        newkeywords = []\n",
    "        keywords = [word[0] for word in topic]\n",
    "        for word in keywords:\n",
    "            # Remove words that contain only digits\n",
    "            if word.isdigit():\n",
    "                continue\n",
    "            # Remove 1-gram if word is included in a 2-gram\n",
    "            elif sum([word in x for x in keywords])==1:\n",
    "                newkeywords.append(word)\n",
    "        newtopics.append(newkeywords[:n_words])\n",
    "    return newtopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9465d724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 keywords: ['apple', 'covid 19', 'health', 'year']\n",
      "Topic 1 keywords: ['apple', 'engineering', 'campus', 'plans']\n",
      "Topic 2 keywords: ['wild', 'proportional', 'exec', 'jan asked']\n",
      "Topic 3 keywords: ['cbs sports', 'positive', 'players', 'josh']\n",
      "Topic 4 keywords: ['students', 'email', 'must', 'drive']\n"
     ]
    }
   ],
   "source": [
    "feature_vecs, vectorizer = vectorize(article_text,vectorizer_type='tfidf')\n",
    "topics = model_topics(feature_vecs,vectorizer,n_topics=5,n_words=15,model_type='lda')\n",
    "deduped_topics = dedupe_topics(topics,n_words=4)\n",
    "for i,keywords in enumerate(deduped_topics):\n",
    "    print('Topic {} keywords: {}'.format(i,keywords))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
