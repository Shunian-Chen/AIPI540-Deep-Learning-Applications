{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/jjr10/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "#!python -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "if not os.path.exists('../data'):\n",
    "    os.mkdir('../data')\n",
    "if not os.path.exists('../data/agnews'):\n",
    "    url = 'https://storage.googleapis.com/aipi540-datasets/agnews.zip'\n",
    "    urllib.request.urlretrieve(url,filename='../data/agnews.zip')\n",
    "    zip_ref = zipfile.ZipFile('../data/agnews.zip', 'r')\n",
    "    zip_ref.extractall('../data/agnews')\n",
    "    zip_ref.close()\n",
    "\n",
    "train_df = pd.read_csv('../data/agnews/train.csv')\n",
    "test_df = pd.read_csv('../data/agnews/test.csv')\n",
    "\n",
    "# Combine title and description of article to use as input documents for model\n",
    "train_df['full_text'] = train_df.apply(lambda x: ' '.join([x['Title'],x['Description']]),axis=1)\n",
    "test_df['full_text'] = test_df.apply(lambda x: ' '.join([x['Title'],x['Description']]),axis=1)\n",
    "\n",
    "# Create dictionary to store mapping of labels\n",
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a couple of the documents\n",
    "for i in range(5):\n",
    "    print(train_df.iloc[i]['full_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence,method='spacy'):\n",
    "# Tokenize and lemmatize text, remove stopwords and punctuation\n",
    "\n",
    "    punctuations = string.punctuation\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    if method=='nltk':\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        tokens = nltk.word_tokenize(sentence,preserve_line=True)\n",
    "        tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "        tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "        tokens = \" \".join([i for i in tokens])\n",
    "    else:\n",
    "        with nlp.select_pipes(enable=['tokenizer','lemmatizer']):\n",
    "            tokens = nlp(sentence)\n",
    "        tokens = [word.lemma_.lower().strip() for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in stopwords and word not in punctuations]\n",
    "        tokens = \" \".join([i for i in tokens])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:57<00:00, 2105.24it/s]\n",
      "100%|██████████| 7600/7600 [00:03<00:00, 2028.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize training set text\n",
    "tqdm.pandas()\n",
    "train_df['processed_text'] = train_df['full_text'].progress_apply(lambda x: tokenize(x,method='nltk'))\n",
    "\n",
    "# Tokenize test set text\n",
    "tqdm.pandas()\n",
    "test_df['processed_text'] = test_df['full_text'].progress_apply(lambda x: tokenize(x,method='nltk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features using word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(train_data, test_data, ngram_range, method='count'):\n",
    "    if method == 'tfidf':\n",
    "        # Create features using TFIDF\n",
    "        vec = TfidfVectorizer(ngram_range=ngram_range)\n",
    "        X_train = vec.fit_transform(train_df['processed_text'])\n",
    "        X_test = vec.transform(test_df['processed_text'])\n",
    "\n",
    "    else:\n",
    "        # Create features using word counts\n",
    "        vec = CountVectorizer(ngram_range=ngram_range)\n",
    "        X_train = vec.fit_transform(train_df['processed_text'])\n",
    "        X_test = vec.transform(test_df['processed_text'])\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "method = 'tfidf'\n",
    "ngram_range = (1, 2)\n",
    "X_train,X_test = build_features(train_df['processed_text'],test_df['processed_text'],ngram_range,method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "We will use a simple softmax regression as our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classification model using logistic regression classifier\n",
    "y_train = train_df['Class Index']\n",
    "logreg_model = LogisticRegression(solver='saga')\n",
    "logreg_model.fit(X_train,y_train)\n",
    "preds = logreg_model.predict(X_train)\n",
    "acc = sum(preds==y_train)/len(y_train)\n",
    "print('Accuracy on the training set is {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set is 0.919\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on the test set\n",
    "y_test = test_df['Class Index']\n",
    "test_preds = logreg_model.predict(X_test)\n",
    "test_acc = sum(test_preds==y_test)/len(y_test)\n",
    "print('Accuracy on the test set is {:.3f}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
